{
  "hash": "f07a54871c04e8e46db3613f6259dfbf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"BigQuery\"\nsubtitle: \"Centralized database for integrated expedition data\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-location: right\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\n\n\n\n## Overview\n\nThe Pristine Seas Science Database is a centralized, modular system hosted in Google BigQuery that serves as the foundation of our data management infrastructure. This system integrates ecological data collected across more than a decade of scientific expeditions, supporting high-integrity, reproducible research on marine biodiversity and informing global ocean conservation policy.\n\n#### Why BigQuery?\n\nGoogle BigQuery offers several advantages for marine ecological data:\n\n  - Scalability: Handles our growing dataset spanning 40+ expeditions without performance degradation\n  - Integration: Seamlessly connects with our R-based analysis workflows\n  - Collaboration: Enables standardized access across our distributed research team\n  - Future-proofing: Provides a robust platform that can evolve with our research needs\n\nNote: BigQuery is were Global Fishing Watch data lives and we have access to the raw backend data. \n\n#### FAIR Data Principles\n\nOur database implementation adheres to the FAIR data principles:\n\n::: {.grid}\n::: {.g-col-3}\n#### Findable\n- Unique and unified IDs\n- WoRMS-linked taxonomy\n- Consistent spatial hierarchy\n:::\n\n::: {.g-col-3}\n#### Accessible\n- Query-ready BigQuery tables\n- Controlled access and permissions\n- Varied connection options\n:::\n\n::: {.g-col-3}\n#### Interoperable\n- Spatial reference system\n- Harmonized taxonomic backbone\n- Consistent units (cm, g, m²)\n:::\n\n::: {.g-col-3}\n#### Reusable\n- Comprehensive metadata\n- Reproducible code\n- Clear provenance6\n:::\n:::\n\n## Database Architecture\n\nThe Pristine Seas Science Database is organized around two major dataset groups that balance flexibility with consistency:\n\n1. **Method Datasets**: Specific to each survey technique (UVS, BRUVS, eDNA, etc.)\n2. **Reference Datasets**: Shared taxonomic, spatial, and lookup tables providing a unified backbone\n\n### Database Organization\n\n\n```{mermaid}\ngraph LR\n    %% Main project node\n    pristine[\"pristine-seas\"] \n    \n    %% First level - dataset categories\n    reference_group[\"Reference Datasets\"]\n    method_group[\"Method Datasets\"]\n    \n    %% Reference datasets\n    exp[\"expeditions/\"]\n    tax[\"taxonomy/\"]\n    look[\"lookup/\"]\n    \n    %% Method datasets\n    uvs[\"uvs/\"]\n    pbruv[\"pbruv/\"]\n    sbruv[\"sbruv/\"]\n    edna[\"edna/\"]\n    other[\"Other Methods...\"]\n       %% UVS tables\n    sites[\"sites\"]\n    stations[\"stations\"]\n    \n    %% Fish transect tables\n    blt_group[\"Fish Belt Transect Tables\"]\n    blt1[\"blt_stations\"]\n    blt2[\"blt_observations\"]\n    blt3[\"blt_biomass_by_taxa\"]\n    \n    %% LPI tables\n    lpi_group[\"Benthic LPI Tables\"]\n    lpi1[\"lpi_stations\"]\n    lpi2[\"lpi_counts\"]\n    lpi3[\"lpi_cover_by_taxa\"]\n    \n    %% Taxonomy tables\n    fish[\"fish\"]\n    benthos[\"benthos\"]\n    inverts[\"inverts\"]\n    \n    %% Expeditions tables\n    info[\"info\"]\n    exp_sites[\"sites\"]\n    \n    %% Connections\n    pristine --> reference_group\n    pristine --> method_group\n    \n    reference_group --> exp\n    reference_group --> tax\n    reference_group --> look\n    \n    method_group --> uvs\n    method_group --> pbruv\n    method_group --> sbruv\n    method_group --> edna\n    method_group --> other\n    \n    uvs --> sites\n    uvs --> stations\n    uvs --> blt_group\n    uvs --> lpi_group\n    \n    blt_group --> blt1\n    blt_group --> blt2\n    blt_group --> blt3\n    \n    lpi_group --> lpi1\n    lpi_group --> lpi2\n    lpi_group --> lpi3\n    \n    tax --> fish\n    tax --> benthos\n    tax --> inverts\n    \n    exp --> info\n    exp --> exp_sites\n    \n    %% Styling with improved visibility and contrast\n    classDef root fill:#004165,color:#ffffff,stroke:#002e48,stroke-width:2px,rx:8,ry:8\n    classDef group fill:#f8f9fa,color:#000000,stroke:#343a40,stroke-width:1px,stroke-dasharray: 5 5,rx:5,ry:5\n    classDef refDataset fill:#d4edda,color:#000000,stroke:#28a745,stroke-width:2px,rx:8,ry:8\n    classDef methodDataset fill:#cce5ff,color:#000000,stroke:#0d6efd,stroke-width:2px,rx:8,ry:8\n    classDef table fill:#ffffff,color:#000000,stroke:#6c757d,stroke-width:1px,rx:3,ry:3\n    classDef tableGroup fill:#f8f9fa,color:#000000,stroke:#343a40,stroke-width:1px,rx:3,ry:3\n    \n    class pristine root\n    class reference_group,method_group group\n    class exp,tax,look refDataset\n    class uvs,pbruv,sbruv,edna,other methodDataset\n    class sites,stations,info,exp_sites,blt1,blt2,blt3,lpi1,lpi2,lpi3,fish,benthos,inverts table\n    class blt_group,lpi_group tableGroup\n```\n\n\nFor the complete database documenation please refer to the [Pristine Seas Database Documentation](https://pristine-seas.github.io/legacy-db/).\n\n### Common Dataset Structures\n\nEach method dataset follows a consistent pattern with tables organized into logical categories that support various analytical needs. This standardized structure enables efficient querying, cross-method integration, and reproducible science.\n\n::: {.callout-note appearance=\"simple\"}\n## Standard Table Types\n\n- **Site Tables** (`sites`, `[method]_sites`)\n  - One row per survey site or deployment\n  - Contains geographic coordinates, habitat descriptions, and sampling context\n  - Links to the shared `expeditions.sites` table for spatial hierarchy\n  \n- **Station Tables** (`[method]_stations`)\n  - One row per sampling unit (e.g., depth strata, replicate)\n  - Includes sampling parameters, environmental conditions, and method-specific metadata\n  - Functions as the primary unit for analysis and comparison\n  \n- **Observation Tables** (`[method]_observations`)\n  - Contains individual records (e.g., fish counts, benthic points, video annotations)\n  - Stores raw QAQC'd ecological data with taxonomic identifications\n\n- **Summary Tables** (`[method]_[metric]_by_[dimension]`)\n  - Aggregated analysis-ready metrics (e.g., biomass by species, cover by category)\n  - Pre-calculated to standardize common analytical outputs\n  - Enables efficient cross-site and cross-method comparisons\n:::\n\n\n## Data Flow: Field to Database Pipeline\n\nThe integration of expedition data into BigQuery follows a standardized workflow that ensures data quality and consistency:\n\n\n```{mermaid}\nflowchart LR\n    subgraph Field[Field]\n        direction TB\n        A[Collection] --> B[Validation]\n        B --> C[Ship Storage]\n    end\n    \n    subgraph Process[Processing]\n        direction TB\n        D[Drive Backup] --> E[Pipeline Processing]\n        E --> F[Quality Control]\n    end\n    \n    subgraph Database[Database]\n        direction TB\n        G[BigQuery Ingestion] --> H[Analysis-Ready Data]\n    end\n    \n    Field --> Process\n    Process --> Database\n    \n    classDef field fill:#004165,color:#ffffff,stroke-width:1px\n    classDef process fill:#8EBDC8,color:#000000,stroke-width:1px\n    classDef db fill:#E63946,color:#ffffff,stroke-width:1px\n    \n    class Field,A,B,C field\n    class Process,D,E,F process\n    class Database,G,H db\n```\n\n\n#### Process Steps\n\n1. **Field Collection**: Researchers collect and record data using standardized methods and digital fieldbooks\n2. **Initial Validation**: Field-level data checks ensure completeness and quality\n3. **NAS Storage**: All expedition data is securely stored on the ship's Network Attached Storage\n4. **Google Drive Backup**: Post-expedition, data is organized and backed up to Google Drive\n5. **Pipeline Processing**: Each method's data undergoes standardized processing via code in expedition repositories\n6. **Quality Assurance**: Automated and manual checks verify data integrity\n7. **Database Ingestion**: Processed data is ingested into the appropriate BigQuery tables\n8. **Analysis**: Data becomes available for standardized analysis workflows\n\n## Access and Use\n\nThe Pristine Seas Science Database is designed to be accessible through multiple interfaces.\n\n### Rstudio\n\nThe most common way to interact with our BigQuery database is through R, using the familiar tidyverse workflow.\n\n#### Establishing a Connection\n\nSetting up a connection is straightforward:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Load required packages\nlibrary(DBI)\nlibrary(bigrquery)\nlibrary(tidyverse)\n\n# Create a database connection\nbq_connection <- DBI::dbConnect(bigrquery::bigquery(),\n                      project = \"pristine-seas\",\n                      billing = \"pristine-seas\")\n```\n:::\n\n\nThis code establishes a connection to the entire database, allowing you to explore datasets and tables directly from the **Connections** pane in RStudio. The first time you run this code, you'll be prompted to authenticate with your Google account.\n \n#### Using dplyr Verbs\n\nThe real power comes from using familiar dplyr verbs directly with BigQuery tables—no SQL knowledge required:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndf <- tbl(bq_connection, \"taxonomy.fish\") |> \n  group_by(family, genus) |> \n  summarize(n_taxa = n_distinct(accepted_aphia_id),\n            .groups = \"drop\") |> \n  arrange(desc(n_taxa)) |> \n  head(30) |> \n  collect()\n  \ndf |> \n  ggplot()+\n  geom_col(aes(x = fct_reorder(family, n_taxa, sum), \n               y = n_taxa, \n               fill = genus), \n           show.legend = T)+\n  theme_minimal()+\n  coord_flip()+\n  labs(x = \"\", y = \"# Distinct Taxa\", fill = \"\",\n       title = \"Number of fish taxa in taxonomy.fish table\",\n       subtitle = \"By family and genus\")+\n  paletteer::scale_fill_paletteer_d(\"ggsci::default_igv\")\n```\n\n::: {.cell-output-display}\n![](bigquery_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWhat makes this approach powerful:\n\n1. **BigQuery does the heavy lifting** – All filtering, grouping, and summarizing happens in the database\n2. **Only results are transferred** – Data never loads into memory until you call `collect()`\n3. **Familiar syntax** – The same dplyr verbs you already use with local data frames\n4. **Readable code** – Complex queries expressed in clean, maintainable R code\n\n\n### Google BigQuery Console\n\nThe database can also be accessed directly through the Google BigQuery Console:\n\n1. Visit [console.cloud.google.com/bigquery](https://console.cloud.google.com/bigquery)\n2. Navigate to the `pristine-seas` project\n3. Use the query editor to write and execute SQL queries\n4. Explore tables, schemas, and query history\n\nThe console provides a user-friendly interface for exploring table structures, examining data samples, and running ad-hoc queries without writing code.\n\n### Access Management\n\nAccess to the Pristine Seas Science Database is managed through Google Cloud IAM:\n\n- **Team Members**: Full read access to all datasets\n- **Collaborators**: Read access to specific datasets relevant to their work\n- **Partners**: Access via shared exports or temporary read credentials\n- **Public**: Access to published, non-sensitive data via data packages (still TBD)\n\nTo request access, contact the database administrator with your Google account email and purpose.\n\n### Best Practices\n\nWhen working with the Pristine Seas Science Database:\n\n1. **Minimize data transfer**: Filter data in BigQuery before collecting to R\n2. **Use primary keys**: Join tables using established keys (`ps_station_id`, `ps_site_id`)\n3. **Reproducible queries**: Document your queries in Quarto documents\n4. **Analysis patterns**: Build on established workflows in expedition repositories\n\n## Maintenance\n\nThe Pristine Seas Science Database is actively being developed and continously improved to meet the needs of our team. \n",
    "supporting": [
      "bigquery_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}