[
  {
    "objectID": "field-methods/uvs_overview.html",
    "href": "field-methods/uvs_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Underwater visual surveys (UVS) are the cornerstone of Pristine Seas ecological assessments, providing a comprehensive, quantitative foundation for understanding marine ecosystem structure and function. These standardized methods enable robust comparisons across sites, regions, and time periods, creating a powerful global dataset to inform science-based conservation.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Pristine Seas divers conducting underwater visual surveys. Left: Fish belt transect survey. Right: Benthic survey using line point intercept method.\n\nThrough a multi-method approach, we capture the key components of marine ecosystem health: fish populations, benthic composition, invertebrate communities, and coral recruitment. This holistic assessment provides a scientific basis for identifying conservation priorities and designing effective marine protected areas.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)"
    ]
  },
  {
    "objectID": "field-methods/uvs_overview.html#introduction",
    "href": "field-methods/uvs_overview.html#introduction",
    "title": "Overview",
    "section": "",
    "text": "Underwater visual surveys (UVS) are the cornerstone of Pristine Seas ecological assessments, providing a comprehensive, quantitative foundation for understanding marine ecosystem structure and function. These standardized methods enable robust comparisons across sites, regions, and time periods, creating a powerful global dataset to inform science-based conservation.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Pristine Seas divers conducting underwater visual surveys. Left: Fish belt transect survey. Right: Benthic survey using line point intercept method.\n\nThrough a multi-method approach, we capture the key components of marine ecosystem health: fish populations, benthic composition, invertebrate communities, and coral recruitment. This holistic assessment provides a scientific basis for identifying conservation priorities and designing effective marine protected areas.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)"
    ]
  },
  {
    "objectID": "field-methods/uvs_overview.html#survey-components",
    "href": "field-methods/uvs_overview.html#survey-components",
    "title": "Overview",
    "section": "Survey Components",
    "text": "Survey Components\nThe Pristine Seas underwater visual survey framework integrates four complementary components:\n\nFish Belt Transects: Quantify fish diversity, abundance, and biomass\nLine Point Intercept (LPI): Assess benthic cover and composition\nInvertebrate Counts: Document mobile invertebrate abundance\nCoral Recruit Surveys: Measure coral recruitment and population dynamics\n\nThese components are conducted along the same depth strata but on separate transects that are co-located as closely as possible within the same habitat and site. This approach allows for comprehensive sampling of each ecosystem element.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)"
    ]
  },
  {
    "objectID": "field-methods/uvs_overview.html#survey-structure",
    "href": "field-methods/uvs_overview.html#survey-structure",
    "title": "Overview",
    "section": "Survey Structure",
    "text": "Survey Structure\n\nSites\nEach underwater visual survey location is designated as a site. Sites form the primary organizational unit for our surveys and are carefully selected to represent the full range of marine environments within the study area.\nSite Selection Criteria:\n\nHabitat representation: Major habitat types present in the region (e.g., forereef, lagoon, backreef)\nExposure gradient: From protected/sheltered to exposed locations\nProtection status: Both inside and outside protected areas when applicable\nCommunity input: Locations identified by local knowledge as important\nAccessibility: Practical considerations for safe dive operations\n\n\n\nStations\nEach site contains stations at different depths to capture vertical zonation patterns. Typically, two depths are surveyed at each site and the specific depths surveyed depend on local bathymetry, habitat distribution, and research priorities. In some locations, only one depth strata may be present or accessible. In this cases, the team may double the sampling effort at the single station.\n\n\n\n\n\n\nStandard UVS depth strata\n\n\n\nEvery survey station is assigned to one of three standardized depth strata with corresponding suffixes that build the station label:\nDepth strata and suffixes:\n\nSupershallow (≤ 6 m) → 05m\n\nShallow (7–14 m) → 10m\n\nDeep (≥ 15 m) → 20m\n\nStation ID structure: [ISO3]_[YEAR]_uvs_[SITE]_[Depth_suffix]\nExamples:\n\nFirst survey of Fiji’s 2025 expedition at 5.8 m → FJI_2025_uvs_001_05m\nFirst survey of Fiji’s 2025 expedition at 12 m → FJI_2025_uvs_001_10m\n\nThis scheme ensures consistent stratification across all analyses, even with natural variation in survey depth.\n\n\n\n\nTransects\nAt each station, survey methods use multiple standardized transects at consistent depth contours to derive robust station averages. These function as pseudoreplicates rather than independent samples, improving precision and accounting for small-scale spatial variation. Detailed specifications are provided in each method’s dedicated section.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)"
    ]
  },
  {
    "objectID": "field-methods/uvs_overview.html#field-operations",
    "href": "field-methods/uvs_overview.html#field-operations",
    "title": "Overview",
    "section": "Field Operations",
    "text": "Field Operations\nThe Pristine Seas underwater visual surveys involve careful coordination between multiple divers conducting complementary assessments at each site. This orchestrated approach maximizes data collection efficiency while maintaining scientific rigor and preventing interference between methods.\n\nDive Team\n\n\nFish Survey Team\n\n2 divers\nWork independently at each depth strata\nRotate depths between dives\n\n\n\nBenthic Survey Unit\n\n3 divers (LPI, Coral, Inverts/Recruits)\nWork as a coordinated unit across both strata\n\n\n\n\n\nSchedule\nThe typical expedition schedule includes three dives per day:\n\n2 morning dives (08:00-12:00)\n1 afternoon dive (14:00-16:00)\n\n\n\nChoreography\n\nSite preparation:\n\nDive boat positions at GPS coordinates\nEntry point marked with mooring or surface marker\nTeam conducts pre-dive briefing\nYSI profile taken and surface eDNA samples collected if applicable\n\nDescent sequence:\n\nTeam descends together to the deeper station and assess conditions\nIf workable, teams separate to respective survey areas/strata\n\nFish Survey:\n\nEach fish diver works at their assigned depth (2-3 transects)\nCollects photographic records of site and notable, hard to id species\nNotes noteworhtly off transect observations\n\nBenthic Survey:\n\nTeam descends together to the 20m depth stratum\nIf eDNA collection is planned, recruit/invert diver collects water samples first\n50m transect line deployed by benthic diver.\nBenthic diver (LPI):\n\nStarts at 50m mark\nWorks toward 0m recording points every 20cm\n\nCoral diver:\n\nStarts at 0m mark\nWorks toward 50m identifying coral species at points\n\nRecruit/invert diver:\n\nStarts at 45m mark with quadrat surveys (every 5m)\nAfter completing quadrats, stashes equipment\nReturns along transect recording invertebrates\n\nCompletion and transition:\n\nDivers meet at transect end\nTeam recovers transect reel and equipment\nGroup ascends together to 10m depth stratum\nEntire process repeated at 10m depth\n\n\nExit coordination:\n\nAll divers return to mooring/entry point\nTeam conducts safety stop together\nSurfaces at initial entry point\n\nPost-dive procedures:\n\nTeam leader completes site documentation\nEach diver completes method-specific datasheets\nEquipment prepared for next dive\nData entered into digital format at the end of the day.\n\n\nThis coordinated approach ensures that comprehensive ecosystem data is collected at each station while maintaining methodological consistency and diver safety.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)"
    ]
  },
  {
    "objectID": "field-methods/uvs_overview.html#documentation-and-quality-control",
    "href": "field-methods/uvs_overview.html#documentation-and-quality-control",
    "title": "Overview",
    "section": "Documentation and Quality Control",
    "text": "Documentation and Quality Control\n\nUVS Sites Fieldbook\nThe UVS Sites Fieldbook is the primary reference for recording site-specific information and survey activities during expeditions. Maintained by the dive team lead, it serves as the authoritative source for cross-referencing UVS datasets and ensuring data consistency.\nDaily records include:\n\nLocation Details: Region, subregion, locality, site name\n\nAccurate Coordinates: GPS coordinates (WGS84)\n\nSite Conditions: Habitat type and exposure level\n\nProtocol Completion: Survey methods executed and any deviations noted\n\nField Notes: Notable observations, incidents, and contextual informatio\n\nAll habitat types and exposure levels must be recorded using the controlled vocabulary specified below. This standardization is critical for understanding ecological patterns, enabling comparisons across sites, and assessing habitat-specific community structure and condition.\n\nHabitat types\nThis vocabulary standardizes the classification of benthic environments encountered during underwater visual surveys. It includes coral reef zones, temperate habitats, and other dominant seafloor types relevant to ecological analysis.\n\n\n\n\n\n\nHabitats\n\n\n\nHabitat\n\nfore_reef — Seaward-facing outer slope of coral reef\n\nback_reef — Lagoon-facing side behind the reef crest\n\nreef_flat — Horizontal shallow zone near the crest\n\npatch_reef — Isolated reef outcrop within lagoon or sand plain\n\npinnacle_reef — Steep-sided, often isolated coral structures\n\nrocky_reef — Non-coral reef formed from rock, with reef biota\n\nreef_pavement — Flat, low-relief hard-bottom reef surface\n\nchannel_pass — Channel through reef system with strong flow\n\nwall — Vertical or steep reef drop-off\n\nkelp_forest — Dense canopy-forming macroalgae habitat\n\nseagrass — Dominant seagrass bed habitat\n\n\n\n\n\n\nExposure\nThis classification describes the hydrodynamic setting of a site, based on relative exposure to wave energy, swell, or current. It helps interpret ecological patterns linked to environmental forcing.\n\n\n\n\n\n\nExposure\n\n\n\nExposure\n\nwindward — Exposed to prevailing swell or wind; typically high wave energy and surge\n\nleeward — Sheltered side of reef or island protected from prevailing swell\n\nlagoon — Located inside a reef or atoll, generally protected from direct wave action\n\nchannel — Within a reef pass or surge channel; often features strong tidal currents\n\nsheltered — Protected from wave energy by a bay or other geomorphic feature\n\nexposed — Generally high energy due to wave or swell exposure; used where windward/leeward doesn’t apply\n\nunknown — Exposure not determined or insufficient data to classify\n\n\n\n\n\nProtection\nThe dive team lead is responsible for recording the protection status of each UVS site in the fieldbook. This information is critical for evaluating the conservation context of survey data and assessing MPA effectiveness, particularly for small local MPAs. We record protection status using two fields:\n\nin_mpa: Indicates whether the site is within a protected area (e.g., MPA, no-take zone) or not.\nmpa_notes: A free-text field to specify the MPA name, protection type (e.g., no-take, seasonal closure), and any local or jurisdictional notes.\n\n\n\n\nPhotographic records\nPhotographic documentation is a core component of the UVS methodology. It provides visual evidence to support species identification, habitat classification, and ecological assessments.\nEach diver is responsible for capturing clear, high-quality images and delivering two outputs:\n\nComplete photo set: All images, organized in folders by station ID, following standard file naming conventions.\nCurated selection: A subset of the best images, labeled with species names, habitat types, or notable observations. These images are intended for reference libraries, reports, and outreach materials.\n\nRefer to the Media Management section for detailed guidance on file naming conventions, metadata standards, and best practices for imagery organization and submission.\n\n\nQuality Assurance Procedures\nTo maintain data integrity across all UVS methods, we implement these key quality control measures:\nSame-Day Data Entry\nRecord all observations immediately after diving while details remain fresh and accurate\nDaily QAQC Review\nSave data to the ship’s NAS for quality checks, including mapping of survey sites to check accuracy and inform planning\nSpecies Verification\nConfirm identifications using photographic evidence and our physical and digital reference library\nTaxonomic Standardization\nMinimize taxonomic errors by using the team’s taxonomic reference list for consistent species identification and database linkage\nSource Documentation\nArchive original field datasheets securely for future reference and verification\nCross-Referenced Records\nEnsure consistency by validating all method-specific data against the UVS Sites Fieldbook",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)"
    ]
  },
  {
    "objectID": "field-methods/uvs_overview.html#integration-for-reef-health-assessment",
    "href": "field-methods/uvs_overview.html#integration-for-reef-health-assessment",
    "title": "Overview",
    "section": "Integration for Reef Health Assessment",
    "text": "Integration for Reef Health Assessment\nThe Pristine Seas methodology integrates data from all survey components to assess ecosystem health through a standardized set of ecological indicators.\n\n\n\n\n\n\n\n\nIndicator\nMetrics\nSignificance\n\n\n\n\nBenthic Composition\n% hard coral and CCA cover\nFoundation of reef structure and growth\n\n\nDegradation Indicators\n% cyanobacteria cover\nEarly warning of environmental stress\n\n\nFish Community\nTotal fish biomass\nOverall ecosystem productivity\n\n\nTrophic Integrity\n% biomass of sharks and top predators\nFood web completeness\n\n\nInvertebrate Status\nDensity of key commercial species\nResource extraction indicator\n\n\nRecovery Potential\nCoral recruitment density\nFuture reef trajectory\n\n\n\n\n\n\n\nThese metrics provide a holistic assessment of marine ecosystem condition, enabling effective conservation planning and monitoring. We are currently developing a composite reef health index that integrates these indicators into a single, standardized measure to better communicate ecosystem status and facilitate comparative analyses.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html",
    "href": "field-methods/uvs_fish_blt.html",
    "title": "Reef Fish",
    "section": "",
    "text": "Reef fish are key indicators of ecosystem health, providing insights into marine productivity, biodiversity patterns, and human impacts. The Pristine Seas fish belt transect (BLT) methodology provides standardized protocols for quantifying fish community structure, biomass distributions, and trophic organization across different habitats and depths. These surveys form a cornerstone of our ecosystem assessments, enabling quantitative comparisons of fish assemblages across geographies.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#introduction",
    "href": "field-methods/uvs_fish_blt.html#introduction",
    "title": "Reef Fish",
    "section": "",
    "text": "Reef fish are key indicators of ecosystem health, providing insights into marine productivity, biodiversity patterns, and human impacts. The Pristine Seas fish belt transect (BLT) methodology provides standardized protocols for quantifying fish community structure, biomass distributions, and trophic organization across different habitats and depths. These surveys form a cornerstone of our ecosystem assessments, enabling quantitative comparisons of fish assemblages across geographies.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#method-overview",
    "href": "field-methods/uvs_fish_blt.html#method-overview",
    "title": "Reef Fish",
    "section": "Method Overview",
    "text": "Method Overview\nThe fish belt transect method employs a standardized visual census approach to systematically document fish abundance, size structure, and species composition. This method has been refined over decades of reef monitoring and provides robust, quantitative data suitable for global comparisons and trend analyses.\n\nKey Features\n\nPaired divers: Two fish specialists conducting parallel surveys\nMulti-size census: Dedicated passes for large and small-bodied fishes\nPrecise size estimation: Length measurements to nearest centimeter\nComplete taxonomic assessment: All visible fish species recorded\nBiomass calculation: Size-based biomass estimates for trophic and functional analyses\nDepth stratification: Consistent sampling at standard depth strata\nReplicated design: Multiple transects per station for statistical robustness",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#field-implementation",
    "href": "field-methods/uvs_fish_blt.html#field-implementation",
    "title": "Reef Fish",
    "section": "Field Implementation",
    "text": "Field Implementation\n\nTeam Structure\nFish surveys require dedicated specialists who focus exclusively on fish observations. Unlike the benthic team, which divides labor by taxonomic expertise, the fish team comprises:\n\nTwo fish divers working independently at each depth stratum\nEach diver conducts three replicate transects\nDivers typically alternate between depth strata on successive dives\n\nThis approach maximizes sampling efficiency while ensuring robust sample replication.\n\n\n\n\n\n\nFish Belt Transect Specifications\n\n\n\n\nTransect dimensions: 25 m × 4 m for large fish (≥20 cm TL); 25 m × 2 m for small fish (&lt;20 cm TL)\nReplication: 3 transects per diver per depth stratum\nDepth strata: Typically 10 m and 20 m (with 5 m stratum added when appropriate)\nDirection: Parallel to shoreline at constant depth\nSeparation: 5+ m between adjacent transects\nTotal area: 300 m² for large fish and 150 m² for small fish per station\n\n\n\n\n\nCollection Procedure\n\nSite preparation:\n\nSelect site with appropriate habitat at target depth\nEnsure sufficient separation from benthic team operations\n\nTransect deployment:\n\nDeploy 25 m transect tape parallel to shoreline\nMaintain consistent depth throughout transect length\n\nSurvey execution:\n\nFirst pass (outward swim):\n\nTarget all fish ≥20 cm TL within 2 m on either side of the transect (4 m total width)\nSwim at a consistent pace (~8-10 minutes per transect)\nIdentify all fish to species level\nEstimate total length to nearest cm\nCount individuals and record observations on slate\n\nSecond pass (return swim):\n\nTarget all fish &lt;20 cm TL within 1 m on either side of the transect (2 m total width)\nMaintain consistent pace with careful attention to substrate\nIdentify all fish to species level\nEstimate total length to nearest cm\nCount individuals and record observations on slate\n\n\nRepeat process:\n\nComplete three replicate transects at the same depth stratum\nMaintain adequate separation between transects (minimum 5 m)\nEnsure transects sample representative and consistent habitat\n\nPhoto documentation:\n\nPhotograph unusual or difficult-to-identify species\nDocument general habitat conditions\nCapture images of notable aggregations or behaviors\n\nSurvey completion:\n\nConduct general site exploration if time permits\nNote presence of significant species not observed on transects\nJoin the benthic team and exit the water together\n\n\n\nAccurate size estimation is critical for biomass calculation and population structure assessment. Fish divers use several strategies to maintain accuracy:\n\nRegular calibration: Practice with objects of known size before each expedition\nReference standards: Use dive slate markings for direct comparison\nProgressive bracketing: Categorize fish into broad size groups initially, then refine by specific cm\nConsistency checks: Compare size estimates between divers periodically\nPhoto validation: Use photography to validate sizes of unusual specimens",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#taxonomic-framework",
    "href": "field-methods/uvs_fish_blt.html#taxonomic-framework",
    "title": "Reef Fish",
    "section": "Taxonomic Framework",
    "text": "Taxonomic Framework\nPrecise species identification and classification are fundamental to the fish survey methodology. Pristine Seas uses a comprehensive taxonomic system that balances field practicality with scientific rigor. This framework connects underwater observations to global taxonomic databases, with the World Register of Marine Species (WoRMS) AphiaID serving as the authoritative identifier throughout all datasets.\n\nIdentification Process\nOur divers achieve species-level identification for over 90% of individuals in real-time, using a structured approach for challenging identifications:\n\nField observation: Record distinguishing features when species identity is uncertain\nReference consultation: Compare observations with identification guides post-dive\nTeam verification: Review difficult identifications collectively\nPhoto documentation: Capture images of unusual specimens for later analysis\n\n\n\n\n\n\n\nIdentification Resources\n\n\n\nField teams use multiple resources to support accurate species identification:\n\nPre-expedition training: Review regional species lists and identification challenges\nField guides: Region-specific printed and digital references available onboard\nDigital libraries: Offline mobile applications with comprehensive visual databases\nPhotography: Systematic image collection of uncertain identifications\nPost-dive consensus: Regular team discussions to standardize identification approaches\n\n\n\n\n\nField Code System\nFor efficient underwater data collection and rapid at-sea data entry, Pristine Seas employs a specialized field code system. These codes serve as temporary identifiers that link field observations to formal taxonomy.\n\n\n\n\n\n\nTaxon Code Structure\n\n\n\nOur field code (taxon_code) system uses a consistent format that makes underwater data recording efficient while maintaining taxonomic precision.\nSpecies-Level Codes\nGEN2.SPEC4 — First 2 letters of genus + first 4 of species (uppercase)\n\nAcanthurus tristis → AC.TRIS\nApogon tricolor → AP.TRIC\nAnthias tricolor → AN.TRIC\n\nHandling Duplicates\nWhen multiple species would share the same code:\n\nMost common taxon keeps the default code\nOthers extend the genus or species portion to ensure uniqueness\n\nExamples:\n\nApogon tristis → AP.TRIS\nAplodactylus tristis → APL.TRIS\nLabroides bilineatus → LA.BILI\nLabroides bilinearis → LA.BILIN\n\nGenus and Family Codes\nUsed when species-level identification isn’t possible:\n\nGenus → GEN4.SP (e.g., Labroides sp. → LABR.SP)\nFamily → FAM4.SPP (e.g., Labridae spp. → LABR.SPP)\n\nHybrids\nHybrid taxa use extended format: GEN2.SPxSP\nExamples:\n\nAcanthurus achilles × nigricans → AC.ACxNI\nAcanthurus olivaceus × nigricans → AC.OLxNI\n\nThese conventions ensure clean data joins, traceability, and consistent taxonomy across all datasets.\n\n\n\n\nReference Database\nOur two-tier database system translates field observations into analysis-ready data:\n\ntaxonomy.uvs_fish_codes: Maps field codes to formal taxonomy\n\nConnects temporary taxon_code to accepted scientific names\nLinks each code to its authoritative AphiaID\nIncludes regional variants and handles identification edge cases (e.g., hybrids)\nMaintains historical continuity across expeditions\n\ntaxonomy.fish: Comprehensive species trait database with AphiaID as the primary key\n\nTaxonomic identifiers: AphiaID (WoRMS), SpecCode (Fishbase), SIS_ID (IUCN)\nTaxonomic classification: Complete tree from phylum to species\nCommon names: Species and family common names (in English)\nTrophic information: Trophic group and level, feeding guild, and diet composition\nMorphometrics: Length-weight relationships, maximum size, and growth parameters\nEcological context: Habitat associations and fishery importance\nConservation status: IUCN categorization\n\n\nThis integrated system enables seamless workflow from underwater observations to sophisticated ecological analyses while maintaining taxonomic precision and compatibility with global biodiversity databases.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#ecological-classifications",
    "href": "field-methods/uvs_fish_blt.html#ecological-classifications",
    "title": "Reef Fish",
    "section": "Ecological Classifications",
    "text": "Ecological Classifications\nAll fish species in the Pristine Seas database are assigned to standardized trophic groups and functional categories based on their ecological roles. This classification system provides a foundation for analyzing reef community structure and ecosystem function.\n\nTrophic Groups\nWe use five primary trophic categories that reflect feeding ecology and position in the food web:\n\n\n\nHerbivore/Detritivore Species that primarily consume plant material or detritus Examples: Parrotfishes (Scaridae), surgeonfishes (Acanthuridae), rabbitfishes (Siganidae)\n\n\n\nPlanktivore Species that primarily consume planktonic organisms Examples: Fusiliers (Caesionidae), chromis (Pomacentridae), anthias (Anthiadinae)\n\n\n\nLower-Carnivore Species that consume mainly invertebrates and small fishes Examples: Wrasses (Labridae), snappers (Lutjanidae), emperors (Lethrinidae)\n\n\n\nTop-Predator Non-shark predatory species primarily consuming other fishes Examples: Groupers (Epinephelidae), barracuda (Sphyraenidae), jacks (Carangidae)\n\n\n\nShark All sharks, regardless of feeding ecology Examples: Reef sharks (Carcharhinidae), nurse sharks (Ginglymostomatidae), hammerheads (Sphyrnidae)\n\n\n\nThese categories allow us to analyze trophic structure across reef ecosystems and assess the impacts of fishing pressure, which typically affects top predators and sharks first, followed by lower trophic levels.\n\n\nHerbivore Functional Groups\nHerbivorous fishes play critical roles in coral reef resilience by controlling algae that compete with corals. We further classify herbivores into four functional groups based on their feeding mode:\n\n\n\nBrowsers Selectively feed on macroalgae, removing entire plants Examples: Naso unicornis, many Kyphosus species\n\n\n\nGrazers Crop algal turf, creating patches of cropped substrate Examples: Many Acanthurus species, Zebrasoma\n\n\n\nScrapers Remove algae and associated sediment and detritus Examples: Many Scarinae (parrotfishes)\n\n\n\nExcavators Remove chunks of substrate while feeding, creating space for coral recruitment Examples: Large-bodied Scarinae, Chlorurus species\n\n\n\nThis functional classification enables detailed analysis of how different types of herbivores contribute to reef processes, particularly coral-algal dynamics and reef recovery potential after disturbances.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#data-workflow",
    "href": "field-methods/uvs_fish_blt.html#data-workflow",
    "title": "Reef Fish",
    "section": "Data Workflow",
    "text": "Data Workflow\nThe Pristine Seas fish survey methodology follows a systematic workflow from field collection to analysis, ensuring data quality and scientific rigor at each stage.\n\nData Entry\nFish survey data are recorded on underwater slates during dives and transferred to standardized Excel fieldbooks on the same day. This immediate transcription preserves data accuracy while memories are fresh. The ISO3_YEAR_fish_fieldbook.xlsx serves as the primary data repository for each expedition with four key components:\n\n\n\n\n\n\nFieldbook Structure\n\n\n\n\nReadme: Expedition details, data entry protocols, and guidelines\nStations: Site information including depth, survey effort, and habitat type\nObservations: Individual fish records with taxonomic codes, counts, and size estimates\nSpecies: Reference list of fish species with taxonomic information\n\n\n\nThe observations sheet includes built-in validation rules that flag potential errors such as:\n\nSpecies codes not in the reference list\nSize estimates exceeding known maximum lengths\nMissing or inconsistent data fields\n\nThis real-time validation enables immediate correction while field observations remain clear in divers’ memories.\n\n\nProcessing Pipeline\nOur data processing transforms raw field observations into analysis-ready datasets through four sequential phases:\n\n1. Consolidation and Standardization\nFirst, we combine data from multiple divers and expedition legs into a unified dataset:\n\nConvert field codes to accepted scientific names and AphiaIDs\nStandardize size measurements and abundance records\nAdd spatial metadata from the site documentation\nJoin species traits from the taxonomy.fish reference table using AphiaID\nCreate a complete dataset that preserves all original observations\n\n\n\n2. Quality Control\nNext, we verify data quality through a systematic review process:\n\nAnalyze length distributions to identify unlikely size measurements\nCompare species lists between divers and stations for consistency\nCheck geographic ranges against known species distributions\nVerify taxonomic classifications against global databases\n\nPotential issues are flagged for expert review rather than automatically removed, preserving data integrity while ensuring scientific accuracy.\n\n\n3. Biomass Calculation\nWe then transform fish lengths into biomass estimates using the standard length-weight relationship:\n\\[W = a \\times L^b\\]\nWhere \\(W\\) is weight in grams, \\(L\\) is total length in centimeters, and \\(a\\) and \\(b\\) are species-specific constants from our reference database. For species without specific parameters, we use coefficients from closely related species or family-level relationships.\nA critical step in this process is the addition of zero values for species that were not observed on a particular transect but are known to occur in the region. This ensures that absence data is properly incorporated into average calculations, preventing biased estimates of abundance and biomass. Zero values are applied:\n\nAfter taxonomic standardization but before aggregation\nBased on the regional species pool for each location\nOnly for species that could reasonably occur in the sampled habitat type\nPrior to calculating transect-level and station-level metrics\n\nThis process generates standardized metrics including:\n\nFish abundance (individuals/m²) by species and functional group\nBiomass (g/m²) at various taxonomic and ecological levels\nCommunity structure indicators like diversity indices\n\n\n\n4. Database Integration\nFinally, we format the processed data for ingestion into our central database, creating four core output files:\n\nblt_observations: Individual-level records with taxonomic classification and biomass\nblt_biomass_by_taxa: Abundance and biomass aggregated by species and station\nblt_stations: Site-level metadata and summary statistics\nblt_taxa_summary: Regional patterns of occurrence, abundance, and biomass\n\nThese standardized outputs allow consistent analysis across expeditions and regions while maintaining connections to the original field observations.\n\n\n\nExploratory Data Analysis\nOur analysis examines fish communities through multiple complementary perspectives:\n\nCommunity Structure\nWe analyze biodiversity patterns through various metrics including:\n\nSpecies richness and diversity indices\nCommunity composition visualizations (NMDS, cluster analysis)\nDominant species identification and rank-abundance patterns\n\nThese analyses provide insights into ecosystem complexity and community organization at different sites.\n\n\nBiomass and Trophic Organization\nWe examine how biomass is distributed across trophic levels to understand ecosystem function. Key analyses include:\n\nProportional biomass of different trophic groups\nContribution of top predators and sharks to total fish biomass\nComparison with baseline values from remote, unfished locations\nAssessment of key species for local economies and food security\n\nThese patterns reveal fishing impacts and help evaluate ecosystem integrity.\n\n\nSize Structure\nFish size distributions provide insights into population health and fishing pressure:\n\nLength-frequency distributions for target species\nMean size comparisons across protection levels\nSize spectra analysis to detect fishing impacts\nGrowth and recruitment pattern assessment\n\nSize structure analysis is particularly valuable for identifying overfishing signals and evaluating protection effectiveness.\n\n\nSpatial Patterns\nBy analyzing how fish communities vary across space, we identify:\n\nBiomass hotspots and priority conservation areas\nHabitat associations for key species\nDepth-related changes in community structure\nEnvironmental correlates of fish distribution\n\n\n\nCross-Method Integration\nWe combine fish data with other survey components to reveal ecosystem relationships:\n\nCorrelations between coral cover and fish diversity/abundance\nRelationships between herbivorous fish and algal communities\nConnections between benthic habitat complexity and fish biomass\nPotential trophic cascades linking predators, herbivores, and reef algae\n\nThese integrated analyses provide a holistic understanding of reef ecosystem function and reveal the ecological processes that drive reef health and resilience.\nThrough these complementary analytical approaches, we transform field observations into ecological insights that inform conservation planning and contribute to our understanding of marine ecosystems—from individual reefs to global patterns.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#limitations",
    "href": "field-methods/uvs_fish_blt.html#limitations",
    "title": "Reef Fish",
    "section": "Limitations",
    "text": "Limitations\nWhile belt transects are an efficient and widely used method for assessing reef fish communities, several limitations should be considered:\n\nDiver influence: Some fish species actively avoid or are attracted to divers\nCryptic species: Small, hidden, or nocturnal fish are typically underrepresented\nSpatial variability: Fish distribution patchiness requires adequate replication\nTemporal variation: Fish communities change throughout the day and seasons\nObserver differences: Individual divers may vary in species identification and size estimation\nDepth constraints: Standard SCUBA limits surveys to relatively shallow depths\nCoverage limits: Transects sample only a small portion of the total habitat area\n\nWe address these limitations through standardized protocols, thorough diver training, adequate replication, and by combining fish surveys with complementary methods like environmental DNA and baited remote underwater video systems.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/uvs_fish_blt.html#references",
    "href": "field-methods/uvs_fish_blt.html#references",
    "title": "Reef Fish",
    "section": "References",
    "text": "References\nCaldwell ZR, Zgliczynski BJ, Williams GJ, Sandin SA (2016) Reef Fish Survey Techniques: Assessing the Potential for Standardizing Methodologies. PLOS ONE 11(4): e0153066. https://doi.org/10.1371/journal.pone.0153066\nFriedlander AM, Sandin SA, DeMartini EE, Sala E (2010) Spatial patterns of the structure of reef fish assemblages at a pristine atoll in the central Pacific. Marine Ecology Progress Series 410: 219-231. https://doi.org/10.3354/meps08634\nMacNeil MA, Graham NAJ, Cinner JE, Wilson SK, Williams ID, Maina J, Newman S, Friedlander AM, Jupiter S, Polunin NVC, McClanahan TR (2015) Recovery potential of the world’s coral reef fishes. Nature 520(7547): 341-344. https://doi.org/10.1038/nature14358\nMorais RA, Bellwood DR (2018) Global drivers of reef fish growth. Fish and Fisheries 19(5): 874-889. https://doi.org/10.1111/faf.12297",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Reef fish"
    ]
  },
  {
    "objectID": "field-methods/index.html",
    "href": "field-methods/index.html",
    "title": "Overview",
    "section": "",
    "text": "The Pristine Seas Science Team employs a range of complementary research methods to document and study marine ecosystems. Each method provides unique insights into different components of ocean biodiversity, ecological processes, and ecosystem health.\nThis integrated research approach allows us to:\nThese insights are fundamental for effective conservation and our job includes to translate them into actionable strategies for marine protection.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Overview"
    ]
  },
  {
    "objectID": "field-methods/index.html#core-methods",
    "href": "field-methods/index.html#core-methods",
    "title": "Overview",
    "section": "Core Methods",
    "text": "Core Methods\n\n\n\n\nBenthic Surveys\n\nDepths: 0-30m | Target: Benthos\n\n\nPoint-intercept transects\n\n\nInvertebrate counts\n\n\nCoral recruits counts\n\n\nProtocol →\n\n\n\nFish Surveys\n\nDepths: 0-30m | Target: Reef Fish\n\n\nBelt transects\n\n\nSpecies identification\n\n\nSize and biomass estimation\n\n\nProtocol →\n\n\n\n\n\nEnvironmental DNA\n\nDepths: All zones | Target: All taxa\n\n\nWater filtration\n\n\nSequence analysis\n\n\nProtocol →\n\n\n\nSeabed BRUVS\n\nDepths: 10-100m | Target: Reef Predators\n\n\nStereo baited remote video systems\n\n\nRelative abundance (MaxN)\n\n\nLength estimation\n\n\nProtocol →\n\n\n\n\n\nPelagic BRUVS\n\nDepths: Surface | Target: Pelagic Predators\n\n\nStereo, Offshore, mid-water deployments\n\n\nSpecies composition\n\n\nMaxN and length estimates\n\n\nProtocol →\n\n\n\nSeabird Surveys\n\nZone: Surface | Target: Birds\n\n\nCoastal transects\n\n\nAt-sea point counts\n\n\nNesting areas, density estimates, diversity\n\n\nProtocol →\n\n\n\n\n\nMarine Mammal Surveys\n\nZone: Surface | Target: Mammals\n\n\nVisual timed transects\n\n\nAereal surveys\n\n\nPhoto identification\n\n\nProtocol →\n\n\n\nROV Surveys\n\nDepths: to 1000m | Target: Deep biota\n\n\nExploratory dives\n\n\nHorizontal video transects\n\n\nSpecies and habitat documentation\n\n\nProtocol →\n\n\n\n\n\nSubmersible Surveys\n\nDepths: to 1300m | Target: Deep biota\n\n\nExploratory dives\n\n\nHorizontal video transects\n\n\nIn-situ observations and specimen collection\n\n\nProtocol →\n\n\n\nDeep Sea Cameras\n\nDepths: to 6000m | Target: Deep sea biota\n\n\nDeep sea scavengers\n\n\nCommunity composition and relative abyundnce (MaxN)\n\n\nNew records and range extensions\n\n\nProtocol →",
    "crumbs": [
      "Home",
      "Field Methods",
      "Overview"
    ]
  },
  {
    "objectID": "field-methods/index.html#methods-integration",
    "href": "field-methods/index.html#methods-integration",
    "title": "Overview",
    "section": "Methods Integration",
    "text": "Methods Integration\nOur field research strategically integrates multiple assessment methods to provide comprehensive ecosystem insights beyond what individual techniques can reveal. Sampling sites are carefully selected to maximize spatial and temporal alignment across methods, enabling analyses of relationships between different ecosystem components.\nTo ensure seamless data integration, all methods follow standardized protocols including uniform taxonomic references, consistent metadata structures, shared spatial hierarchies, and compatible statistical approaches. This standardization facilitates robust cross-method analyses that reveal critical ecological relationships and functions.\n\nShared Spatial Hierarchy\n\n\n\n\n\n\nHierarchical spatial structure\n\n\n\nOur nested structure organizes all collected data into meaningful spatial contexts:\n\nExpedition: A complete research campaign to a country or target area\nConvention: [ISO3]_[YEAR]\nExamples: PLW_2023, MDV_2024\n\nRegion: Broad geographic or administrative area\nExamples: Temotu Province, Manus Province\n\nSubregion: Intermediate unit like an island, atoll, gulf, or reef complex\nExamples: Duff Islands, Harengan\n\nLocality: Local named feature such as a islet, community, village\nExamples: Taumako, Pinyang island\n\nSite: Specific location where sampling methods are deployed\nConvention: [ISO3]_[YEAR]_[METHOD]_[SITE]\nExamples: PLW_2023_uvs_001, FJI_2023_pbruv_001\n\nStation: A discrete sampling unit within a site, usually depth strata\nConvention: [ISO3]_[YEAR]_[METHOD]_[SITE]_[STATION]\nExamples: PLW_2023_uvs_001_10m, PLW_2023_uvs_001_20m\n\nMeasurement/Observation: Individual data points collected at a station\nExamples: Fish count, Coral cover percentage, remote video footage\n\n\n\n\n\n\n\nThis allows for analyses at multiple scales while maintaining clear relationships between sampling elements.\n\n\n\n\nConsistent Sampling model\nThe table below outlines the spatial sampling structure for each method in the Pristine Seas Science Database. It defines the hierarchical relationship from sites to observations, ensuring clarity and consistency across protocols.\n\n\n\n\n\n\n\n\nPristine Seas Sampling Model\n\n\nSite &gt; Station &gt; Transect &gt; Observation\n\n\nMethod\nSite\nStation\nTransect\nObservation\n\n\n\n\nUVS\nDive survey\nDepth strata\nBLT (3), LPI (1), Inverts (1), Recruits (10)\nCounts, biomass, cover\n\n\neDNA\nWater collection site\nDepth strata\nWater bags or pumps\nSpecies DNA\n\n\nPelagic BRUVS\n5-rig BRUVS deployment\nRig\nN/A\nSpecies ID, length, MaxN\n\n\nSeabed BRUVS\nSingle BRUVS deployment\nSingle station\nN/A\nSpecies ID, length, MaxN\n\n\nSubmersible Surveys\nSub dive\nDepth strata\nVideo transects\nSpecies ID, counts, habitat\n\n\nDeep-Sea Cameras\nSingle camera deployment\nSingle station\nN/A\nSpecies ID, abundance\n\n\nROV Surveys\nROV dive\nDepth strata\nVideo transects\nSpecies ID, counts, habitat\n\n\nYSI Loggers\nLogger deployment\nSingle profile\nN/A\nTemp, salinity, DO, pH\n\n\nBird Surveys\nSurvey point\nSingle survey\nWalking transect or point count\nSpecies counts, behavior\n\n\n\n\n\n\n\nThe Pristine Seas Science Database serves as the central repository for this integrated data architecture, preserving methodological connections while maintaining data integrity. Complete documentation of database structure and integration protocols is available here.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Overview"
    ]
  },
  {
    "objectID": "field-methods/index.html#general-data-worklow",
    "href": "field-methods/index.html#general-data-worklow",
    "title": "Overview",
    "section": "General Data Worklow",
    "text": "General Data Worklow\nThe diagram below illustrates the journey of field data from initial collection to its final integration within our database. This structured workflow ensures that data collected from diverse locations and methods is processed and stored consistently. As our network of collaborating researchers grows, maintaining this standardization is crucial—not only for ease of analysis but also to safeguard the scientific integrity of our collective work.\n\n\nCode\nflowchart LR\n    subgraph Field[\"🔍 Field Collection\"]\n        direction TB\n        A[Data Collection] --&gt; B[Field Validation]\n        B --&gt; C[Daily Entry]\n    end\n    \n    subgraph Process[\"🔄 Data Processing\"]\n        direction TB\n        D[File Consolidation] --&gt; E[Quality Control]\n        E --&gt; F[Integration & Cleaning]\n    end\n    \n    subgraph Analysis[\"📊 Data Analysis\"]\n        direction TB\n        G[Exploratory Analysis] --&gt; H[Statistical Testing]\n        H --&gt; I[Visualization & Reporting]\n    end\n    \n    subgraph Database[\"🗄️ Database Storage\"]\n        direction TB\n        J[Formatting] --&gt; K[BigQuery Tables]\n        K --&gt; L[Cross-Method Integration]\n    end\n    \n    Field --&gt; Process\n    Process --&gt; Analysis\n    Process --&gt; Database\n    Database --&gt; Analysis\n    \n    style Field fill:#004165,color:#ffffff\n    style Process fill:#8EBDC8,color:#000000\n    style Analysis fill:#EA9E8D,color:#000000\n    style Database fill:#F6C141,color:#000000\n\n\n\n\n\nflowchart LR\n    subgraph Field[\"🔍 Field Collection\"]\n        direction TB\n        A[Data Collection] --&gt; B[Field Validation]\n        B --&gt; C[Daily Entry]\n    end\n    \n    subgraph Process[\"🔄 Data Processing\"]\n        direction TB\n        D[File Consolidation] --&gt; E[Quality Control]\n        E --&gt; F[Integration & Cleaning]\n    end\n    \n    subgraph Analysis[\"📊 Data Analysis\"]\n        direction TB\n        G[Exploratory Analysis] --&gt; H[Statistical Testing]\n        H --&gt; I[Visualization & Reporting]\n    end\n    \n    subgraph Database[\"🗄️ Database Storage\"]\n        direction TB\n        J[Formatting] --&gt; K[BigQuery Tables]\n        K --&gt; L[Cross-Method Integration]\n    end\n    \n    Field --&gt; Process\n    Process --&gt; Analysis\n    Process --&gt; Database\n    Database --&gt; Analysis\n    \n    style Field fill:#004165,color:#ffffff\n    style Process fill:#8EBDC8,color:#000000\n    style Analysis fill:#EA9E8D,color:#000000\n    style Database fill:#F6C141,color:#000000\n\n\n\n\n\n\n\nField Collection\nField collection is the foundation of our scientific process. Every observation is documented following standardized protocols to ensure that data is consistent and reliable. We emphasize:\n\nStandardized fieldbooks specifically designed for each method, ensuring structured and error-free data collection.\nConsistent recording of site IDs, coordinates, date, time, and observer names for traceability.\nCommon terminology for habitat types, substrates, and species to maintain comparability across sites.\nPhoto documentation as visual evidence to complement written records.\nSystematic sampling methods that allow data to be compared confidently across different regions.\n\nThese protocols not only enhance the reliability of our own analysis but also make our data easily understandable and usable by other researchers working with us.\n\n\nData Entry\nOnce collected, data is entered into digital formats as soon as possible to prevent loss of detail and reduce errors. Our main practices include:\n\nSame-day data entry to keep information fresh and accurate.\nFieldbooks with built-in error checks to minimize typos and formatting mistakes.\nStandardized file naming: [ISO3]_[YEAR]_[METHOD]_fieldbook.xlsx for easy identification.\nReal-time backups on the ship’s storage system (NAS) to prevent data loss.\nDaily review by a data manager to spot errors early and maintain data quality.\n\nThis structured approach ensures that as more researchers join our efforts, everyone has access to clean, well-organized data that is easy to interpret and integrate across projects. It is our commitment to scientific rigor and transparency.\n\n\nMedia Management\nPhotos and videos serve as both scientific evidence and powerful tools for communicating our work. Consistent management ensures these visual assets remain accessible, properly labeled, and scientifically valuable.\n\n\n\n\n\n\nPhoto Management Best Practices\n\n\n\nGood photo management is essential for documenting our scientific work. Follow these three key practices:\n\nOrganize by Station: Keep all photos in folders organized by station ID so they can be easily found later\nSelect Best Images: Create a collection of high-quality, labeled photos showing species, notable observations, and habitats for our reference library and reports\nUse Consistent Names: Follow our standard naming approach for folders and important images\n\nFolders: [ISO3]-[YEAR]-[METHOD]-[STATION_ID]\nExamples:\n\nQuadrats: data/benthos/recruits/quadrats/PLW-2023-uvs-001-20m/\nFish: data/fish/photos/PLW-2023-uvs-001-20m/\nInverts: data/benthos/inverts/photos/PLW-2023-uvs-001-10m/\nBirds: data/birds/photos/PLW-2023-birds-001/\n\nSelected Images: [ISO3]_[YEAR]_[METHOD]_[STATION_ID]_[DESCRIPTION].jpg\nExamples:\n\nBirds: media/science-collection/birds/PLW_2023_birds_001_frigatebird.jpg\nFish: media/science-collection/fish/PLW_2023_uvs_001_20m_acanthurus_lineatus.jpg\nSubmersible: media/science-collection/sub/PLW_2023_sub_08_gorgonian_fan.jpg\n\nField Photos: Only selected highlights need complete labels; other field photos can remain as-is if stored in properly labeled station folders\nExamples:\n\nQuadrats: data/benthos/recruits/quadrats/PLW-2023-uvs-001-20m/q3_IMG0042.jpg\nBirds: data/birds/photos/PLW-2023-birds-001/canon_2023-08-01_12-00-00.jpg\n\n\n\nSave all photos to the NAS daily. The data manager will transfer everything to Google Drive after the expedition.\n\n\n\n\n\n\n\n\nVideo Footage Best Practices\n\n\n\nManaging footage from remote cameras (BRUVS, deep-sea cameras, ROV, submersible) requires organization to make the most of these valuable recordings. Follow these key practices:\n\nUse a Central Highlights Folder: Save all important video clips and screenshots to the appropriate method’s highlights folder\n\nExamples:\n\nSeabed BRUVS: sbruv/highlights/\nPelagic BRUVS: pbruv/highlights/\nROV: rov/highlights/\nSubmersible: sub/highlights/\nDeep-sea cameras: deepsea/highlights/\n\n\nName Files Consistently: Use our standard format for all highlight files\n\nStandard Format: [ISO3]_[YEAR]_[METHOD]_[STATION_ID]_[DEPTH]_[DESCRIPTION].mp4\nExamples:\n\nSeabed BRUVS: FJI_2025_sbruv_001_40m_grouper_school.mp4\nPelagic BRUVS: FJI_2025_pbruv_003_silky_shark.mp4\nROV: FJI_2025_rov_002_350m_soft_coral_garden.mp4\nSubmersible: FJI_2025_sub_005_800m_deep_reef_community.mp4\nDeep-sea camera: FJI_2025_dscm004_1200m_cusk_eel.mp4\n\n\nUse Standard File Formats: Make sure everyone can open and view your files\n\nVideo Clips: Save as .mp4 format for all video highlights\nScreenshots: Save as .jpg format for all still images\nClip Length: Keep highlight clips between 10-60 seconds\nResolution: Maintain 1080p resolution\n\nInclude Important Details: Make sure others can understand what they’re seeing\n\nInclude depth information for all depth-related methods\nUse descriptive names that indicate content (species, behavior, habitat)\nNote the time code from original footage when possible\n\nCapture These Key Moments:\n\nDiversity and MaxN: Footage showing multiple species or high numbers\nBehaviors: Interesting activities like feeding, schooling, or cleaning\nHabitats: Good examples of different habitat types\nNotable Finds: Rare species, unusual interactions, or new records\n\n\nSave all highlight files to the NAS daily. The data manager will transfer everything to Google Drive after the expedition.\n\n\nWhile more detailed guidelines can be found in the Media Library section, these practices ensure that our photos and videos are properly organized and can be easily connected to our scientific observations.\n\n\nProcessing Pipeline\nWe use method-specific standardized scripts (pipeline_[method].qmd) to transform raw field observations into clean, analysis-ready datasets. Although each method follows a unique pipeline, the overall structure remains consistent, ensuring data integrity, traceability, and seamless integration into the Pristine Seas Science Database.\n\nData Ingestion and Harmonization\n\nData Ingestion: Gather and merge raw field data from multiple expedition legs and contributors.\nStandardization: Apply consistent field names, units, and formats to match database schema.\nSpatial Context: Harmonize geographic data across the established hierarchy:\n\nRegion → Subregion → Locality → Site → Station\n\nTerminology Alignment: Enforce consistent language for habitats, strata, and exposure types.\n\n\n\nQA/QC\n\nSampling Effort Verification: Assess completeness and alignment with sampling protocols.\nExploratory Visualizations: Generate maps and plots to visually inspect patterns.\nSpatial and Temporal Validation: Check station alignment and method overlap across regions.\nData Completeness Assessment: Identify missing values and ensure records are logically complete.\nOutlier and Anomaly Detection: Flag improbable values and check for inconsistencies.\nProtocol Deviation Documentation: Record any departures from standard field protocols.\n\n\n\nTaxonomic Validation\n\nSpecies Identification Checks: Cross-reference observed taxa with the Pristine Seas Taxonomic Database.\nResolve Inconsistencies: Correct synonyms, misidentifications, and hierarchical misalignments.\nHarmonize Across Regions: Ensure consistent naming and classification throughout the dataset.\n\n\n\nEcological Metric Calculation\n\nCompute key ecological indicators at relevant spatial scales:\n\nBiomass (g/m²)\nSpecies Richness\nDiversity Indices (e.g., Shannon, Simpson)\nTrophic Composition\nMaxN (Maximum Number Observed Simultaneously)\n\nGenerate summary tables at multiple ecological and political levels:\n\nTaxa → Station → Site → Subregion → Region\n\nProduce both detailed observation-level tables and summary statistics for downstream analyses.\n\n\n\nOutput Standardization and Database Integration\n\nFile Naming Convention: Apply a consistent format for traceability:\n\n[ISO3]_[YEAR]_[METHOD]_[TYPE].csv\nExamples: PLW_2023_fish_observations.csv, FJI_2025_coral_cover.csv\n\nDatabase Upload: Load processed and validated datasets into the Pristine Seas Science Database.\nMedia Linkage: Attach relevant photos and video highlights for cross-referencing.\n\n\n\n\nExploratory Data Analysis (EDA)\nThe Exploratory Data Analysis (EDA) stage transforms standardized datasets into actionable insights that inform scientific reports and conservation strategies. During this phase, we go beyond surface-level exploration to actively seek indicators of ecosystem health, conservation effectiveness, and environmental threats. Analyses are performed in eda_[method].qmd, with a strong emphasis on generating meaningful ecological narratives and identifying conservation priorities.\n\nData Visualization\n\nCreate maps, summary plots, and interactive visualizations to reveal spatial and temporal patterns.\nGenerate summary statistics for key ecological metrics:\n\nBiomass, species richness, diversity indices, and trophic composition.\n\nVisualize ecological gradients across depth, region, and habitat types.\nExplore habitat-specific assemblages and species distributions to detect community shifts.\n\n\n\nContextual Analysis and Ecological Inference\n\nCompare findings across regions, depth strata, and time periods to identify ecological shifts.\nIntegrate results from multiple survey methods to build a comprehensive view of ecosystem dynamics.\nContextualize observations with known baselines, historical data, and previous expeditions.\nDetect range extensions, new records, and unusual species aggregations.\n\n\n\nConservation Insights\nDuring EDA, we explicitly explore key indicators that reflect ecosystem health, resilience, and human impact. This stage is conservation-driven, aiming to identify both risks and successes:\n\nEcosystem Health Indicators\n\nAssess reef health, predator biomass ratios, and trophic balance as indicators of ecosystem stability.\nIdentify keystone species and their relative abundance to understand ecosystem structure.\n\n\n\nOverfishing Evidence\n\nDetect declines in critical species, particularly large-bodied predators and apex species.\nMonitor shifts in size structure and age classes, signaling fishing pressure.\n\n\n\nClimate Change Impacts\n\nLook for signs of coral bleaching, range shifts, and changes in species composition.\nInvestigate sensitive species loss.\n\n\n\nHabitat Degradation\n\nExplore shifts in benthic cover, including macroalgae dominance, cyanobacteria blooms, and reductions in coral cover.\nIdentify areas with increasing sedimentation or evidence of physical damage (e.g., fishing lines).\n\n\n\nConservation Success Signals\n\nHighlight regions with high biomass, top predator abundance, and balanced trophic structures.\nIdentify MPA effectiveness by comparing protected versus unprotected areas.\n\n\n\nCritical Habitat Mapping\n\nMap species home ranges, migration corridors, and nursery areas.\nIdentify biodiversity hotspots and regions of ecological significance for conservation planning.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Overview"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html",
    "href": "field-methods/benthic_inverts.html",
    "title": "Invertebrates",
    "section": "",
    "text": "Mobile invertebrates are key components of marine ecosystems, serving as both ecological indicators and fulfilling critical functional roles in reef maintenance, herbivory, and nutrient cycling. The Pristine Seas invertebrate survey methodology provides standardized protocols for quantifying the abundance, diversity, and size distribution of conspicuous mobile invertebrates across different habitats and depths. These surveys complement our benthic composition data (LPI) by documenting the mobile fauna that drive essential reef processes and often serve as valuable fishery resources.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#introduction",
    "href": "field-methods/benthic_inverts.html#introduction",
    "title": "Invertebrates",
    "section": "",
    "text": "Mobile invertebrates are key components of marine ecosystems, serving as both ecological indicators and fulfilling critical functional roles in reef maintenance, herbivory, and nutrient cycling. The Pristine Seas invertebrate survey methodology provides standardized protocols for quantifying the abundance, diversity, and size distribution of conspicuous mobile invertebrates across different habitats and depths. These surveys complement our benthic composition data (LPI) by documenting the mobile fauna that drive essential reef processes and often serve as valuable fishery resources.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#method-overview",
    "href": "field-methods/benthic_inverts.html#method-overview",
    "title": "Invertebrates",
    "section": "Method Overview",
    "text": "Method Overview\nThe invertebrate survey method employs a belt transect approach to systematically document conspicuous mobile invertebrates within a defined search area. This standardized approach enables quantitative comparison of invertebrate communities across sites, regions, and time periods.\n\nKey Features\n\nDefined search area: 50 m × 1 m belt transect (50 m²)\nFocus taxa: Echinoderms, mollusks, crustaceans, and other conspicuous invertebrates\nSize measurements: Targeted measurements of commercially important or ecologically significant species\nStandardized effort: Consistent search time and methodology across all stations\nIntegration: Uses the same transect line deployed for LPI and coral recruit surveys",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#field-implementation",
    "href": "field-methods/benthic_inverts.html#field-implementation",
    "title": "Invertebrates",
    "section": "Field Implementation",
    "text": "Field Implementation\n\nTeam Structure\nThe invertebrate survey is conducted by a specialized diver as part of the three-person benthic survey team. This integrated approach maximizes efficiency while ensuring consistent sampling across all benthic components:\n\nOne diver focuses on coral recruits and invertebrates\nAnother conducts the Line Point Intercept (LPI) survey for benthic cover\nA third identifies corals at LPI points to species level\n\nThe recruit/invertebrate specialist completes the coral recruit survey first, followed by the invertebrate survey along the same transect, optimizing dive time while maintaining thorough coverage.\n\n\n\n\n\n\nInvertebrate Survey Specifications\n\n\n\n\nTransect dimensions: 50 m × 1 m (50 m²)\nSearch width: 1 m along one side of the transect line\nDirection: From start (0 m) to end (50 m) of the transect\nDepth strata: Typically 10 m and 20 m (matching other benthic methods)\nTiming: Conducted after completing coral recruit quadrats\n\n\n\n\n\nCollection Procedure\n\nInitial preparation:\n\nAfter completing the coral recruit quadrats, return to the start of the transect\nPrepare slate with data sheet and measuring guides\n\nSurvey execution:\n\nBegin at the 0 m mark and work toward the 50 m mark\nSystematically search 1 m along one side of the transect line\nScan substrate, crevices, and undersides of structures\nMaintain a consistent search speed\n\nData recording:\n\nIdentify all motile invertebrates to species level and count them\nFor culturally and fishery important species, measure sizes in centimeters using slate\nExclude species permanently attached to the seabed (except for scallops, pearl oysters, and giant clams)\nNote unusual behavior or interesting ecological interactions\n\nPhoto documentation:\n\nPhotograph all invertebrates not identified in-situ to species\nCapture additional images of unusual specimens or notable aggregations\nDocument evidence of harvesting or ecological interactions\n\nQuality control notes:\n\nNote any deviations from standard protocol\nDocument search time and any interruptions\nRecord habitat and substrate characteristics that might affect invertebrate distributions\n\n\n\n\nMeasurement Protocols\nFor priority species, standardized measurements are taken using the centimeter-marked edge of the dive slate. Each taxonomic group has specific measurement standards to ensure data consistency:\n\n\n\nGiant Clams Maximum shell length for all Tridacnidae Position recorded as embedded, loose, or partial\n\n\n\nPearl Oysters Shell height including Penguin’s wing oysters Measured from hinge to furthest shell margin\n\n\n\nGastropods Shell width/length for Trochus, Triton, Conch varieties Following SPC guide-specific measurement points\n\n\n\nSea Cucumbers Body length when relaxed for all commercial species Measured without disturbing the animal\n\n\n\nSea Urchins Test diameter without spines for non-boring species Especially important for Tripneustes and Diadema\n\n\n\nLobsters Carapace length for all species Measured from eye orbit to posterior carapace edge\n\n\n\nAdditional species may be included based on regional priorities, following the measurement protocols outlined in the SPC invertebrate measurement guide.\n\nPriority Species Monitoring\nCertain key species warrant additional documentation due to their ecological significance or commercial importance:\n\n\n\n\n\n\nCrown-of-Thorns Starfish (COTS)\n\n\n\nFor each Acanthaster individual encountered, we document:\n\nSize: Diameter from arm tip to opposite arm tip (cm)\nBehavior: Activity coded as feeding (F), moving (M), or resting (R)\nHabitat: Substrate type and coral species association\nAggregation: Number of other individuals within 5m radius\nCondition: Any signs of disease, predation, or stress\n\nThis detailed monitoring helps track potential outbreaks that can significantly impact coral reef health and provides essential data for management interventions.\n\n\n\nAll giant clam species receive special attention with documentation of:\n\nSpecies: Complete species-level identification\nSize: Maximum shell length measured along longest axis\nPosition: Recorded as embedded (E), loose (L), or partially embedded (P)\nHealth: Assessment of bleaching status and physical damage\nHabitat: Substrate type and depth within site\n\nAs iconic and vulnerable species, giant clams serve as important indicators of fishing pressure, environmental conditions, and reef health.\n\nThis comprehensive data collection provides critical insights into the status of invertebrate resources and informs sustainable management strategies.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#taxonomic-framework",
    "href": "field-methods/benthic_inverts.html#taxonomic-framework",
    "title": "Invertebrates",
    "section": "Taxonomic Framework",
    "text": "Taxonomic Framework\n\nTarget Taxa\nThe invertebrate survey counts all motile invertebrates, with special attention to ecologically important and commercially valuable species. While we aim for complete enumeration, the following key groups receive priority focus:\n\n\n\nEchinoderms\n\n\nSea stars (Acanthaster, Linckia)\n\n\nSea urchins (Diadema, Tripneustes)\n\n\nSea cucumbers (all commercial species)\n\n\nFeather stars (crinoids)\n\n\n\n\n\nMollusks\n\n\nGiant clams (Tridacnidae)\n\n\nPearl oysters (including Penguin’s wing)\n\n\nGastropods (Trochus, Triton, Conch)\n\n\nNudibranchs & Cephalopods\n\n\n\n\n\nCrustaceans\n\n\nLobsters (Panulirus, Scyllarides)\n\n\nLarge crabs (Carpilius, Etisus)\n\n\nOther large crustaceans\n\n\n\n\n\n\n\nIdentification Process\nOur taxonomic approach adapts to the challenges of identifying diverse invertebrate taxa in the field. All motile invertebrates are identified to species level whenever possible, with photographs taken for specimens that cannot be immediately identified.\nWhen immediate species-level identification is not possible, we employ a hierarchical approach to taxonomic resolution:\n\nField names: Descriptive terms based on obvious morphological characteristics\n\nExamples: “black sea cucumber with red papillae,” “blue-spotted sea star”\n\nMorphotaxa: Consolidation of field names into taxonomic units with distinguishing features\n\nExamples: “Linckia sp. blue,” “Holothuria sp. black with red”\n\nLowest defensible taxonomic rank: Formal taxonomic assignment based on available evidence\n\nExamples: “Linckia laevigata,” “Holothuria atra”\n\nFunctional group: Classification into ecological functional roles\n\nExamples: “Grazing echinoid,” “Deposit-feeding holothuroid”\n\n\n\n\n\n\n\n\nIdentification Resources\n\n\n\nField teams utilize multiple resources to support accurate species identification:\n\nPre-expedition training: Familiarization with expected regional species\nField guides: Region-specific printed references available onboard\nDigital libraries: Offline electronic identification guides\nPhotographic documentation: Systematic imaging for later verification\nExpert consultation: Review of challenging identifications by taxonomic specialists",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#ecological-classifications",
    "href": "field-methods/benthic_inverts.html#ecological-classifications",
    "title": "Invertebrates",
    "section": "Ecological Classifications",
    "text": "Ecological Classifications\nInvertebrates are classified according to several functional frameworks to enable ecological interpretation beyond simple taxonomic groupings.\n\nFeeding Modes\nWe categorize invertebrates into functional feeding groups that reflect their ecological roles:\n\n\n\nHerbivores / Detritivores Consume algae, detritus, or plant material Examples: Many sea urchins, some sea cucumbers\n\n\n\nActive Suspension Feeders Filter feeders that actively generate a feeding current Examples: Sponges, bivalves, bryozoans, tunicates\n\n\n\nPassive Suspension Feeders Rely on ambient water flow to capture particles Examples: Crinoids, gorgonians, dendrochirotid sea cucumbers\n\n\n\nDeposit Feeders Process sediment for organic material Examples: Many sea cucumbers, some sea stars\n\n\n\nPredators Consume other animals Examples: Crown-of-thorns starfish, many gastropods, octopus\n\n\n\n\n\nResource Categories\nMany invertebrates represent important fishery resources. We classify these according to their value and exploitation status:\n\n\n\nHigh-Value Commercial Primary target species in export fisheries Examples: Sea cucumbers, lobsters, giant clams\n\n\n\nLocal Subsistence Resource Species harvested primarily for local consumption Examples: Trochus, octopus, many crabs\n\n\n\nThese classifications provide a framework for interpreting invertebrate patterns in the context of ecosystem function and human use.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#data-workflow",
    "href": "field-methods/benthic_inverts.html#data-workflow",
    "title": "Invertebrates",
    "section": "Data Workflow",
    "text": "Data Workflow\n\nData Entry\nInvertebrate observations are recorded on underwater slates during dives and transferred to standardized Excel fieldbooks on the same day. This immediate transcription preserves data accuracy while memory is fresh. The ISO3_YEAR_inverts_fieldbook.xlsx serves as the primary data repository for each expedition with three key components:\n\n\n\n\n\n\nFieldbook Structure\n\n\n\n\nReadme: Expedition details, data entry protocols, and guidelines\nStations: Site information including depth, habitat type, and protocol adherence\nObservations: Individual invertebrate records with taxonomic identification, counts, and measurements\n\n\n\nThe observations sheet includes validation rules that flag potential errors such as:\n\nMissing taxonomic information\nUnlikely size measurements\nInvalid station numbers\n\n\n\nProcessing Pipeline\nOur data processing transforms raw field observations into analysis-ready datasets through four sequential phases:\n\n1. File Consolidation\nFirst, we compile data from all divers and expedition legs into a unified dataset:\n\nMerge all invertebrate records from multiple dives and sites\nStandardize taxonomic names using reference taxonomy lists\nAdd metadata from the UVS Sites Fieldbook (coordinates, habitat type, etc.)\nCreate a complete dataset that preserves all original observations\n\n\n\n2. Quality Control\nNext, we verify data quality through a systematic review process:\n\nValidate species identifications for consistency and accuracy\nFlag outliers in size measurements for review\nCross-check invertebrate records with habitat types for ecological coherence\nEnsure complete sampling coverage across all sites\n\nPotential issues are flagged for expert review rather than automatically removed, preserving data integrity while ensuring scientific accuracy.\n\n\n3. Density Calculation\nWe then calculate standardized density metrics for cross-site comparisons:\n\nCompute abundance per unit area (individuals/m²) for each taxon\nGenerate biomass estimates for species with established length-weight relationships\nCalculate summary statistics by taxonomic group, site, and region\n\nA critical step in this process is the addition of zero values for taxa that were not observed on a particular transect but were found elsewhere in the region. This ensures that absence data is properly incorporated when calculating average densities and prevents biased estimates. Zero values are applied:\n\nAfter taxonomic standardization but before aggregation\nBased on the regional species pool established for each survey\nOnly for species that could reasonably occur in the sampled habitat type\nPrior to calculating site-level and region-level metrics\n\n\n\n4. Database Integration\nFinally, we format the processed data for ingestion into our central database, creating four core output files:\n\ninverts_observations: Individual records with standardized taxonomy, counts, and measurements\ninverts_density_by_taxa: Density calculations by taxa and station\ninverts_stations: Station-level metadata and summary statistics (richness, density)\ninverts_taxa_summary: Regional occurrence patterns and average densities by taxa\n\nThese standardized outputs allow consistent analysis across expeditions and regions while maintaining connections to the original field observations.\n\n\n\nExploratory Data Analysis\nOur analytical approach examines invertebrate communities through multiple complementary perspectives:\n\nCommunity Composition\nWe analyze biodiversity patterns and community structure:\n\nSpecies richness and diversity indices across sites and protection levels\nMultivariate analyses (NMDS, cluster analysis) to identify assemblage patterns\nFunctional group proportions to assess ecosystem processes\nIndicator species analysis to identify habitat associations\n\nThese analyses provide insights into the factors structuring invertebrate communities and their responses to environmental gradients.\n\n\nPopulation Patterns\nWe examine population characteristics of key species:\n\nDensity comparisons across sites, habitats, and protection levels\nSize frequency distributions to assess population structure\nBiomass estimates for commercially important species\nRecruitment patterns and juvenile abundance\n\nThese metrics help evaluate population health and the impacts of environmental conditions or harvesting pressure.\n\n\nSpatial Patterns\nBy analyzing invertebrate distributions across space, we identify:\n\nHotspots of diversity and abundance\nHabitat associations for key species\nDepth-related community changes\nGeographic patterns in commercially important taxa\n\nSpatial analyses provide critical information for conservation planning and resource management.\n\n\nCross-Method Integration\nWe combine invertebrate data with other survey components to reveal ecosystem relationships:\n\nCorrelations between benthic cover and invertebrate communities\nPredator-prey relationships between fish and invertebrates\nPotential impacts of invertebrate grazers on coral recruitment\nComparisons between visual surveys and eDNA detection of invertebrate diversity\n\nThese integrated analyses provide a holistic understanding of reef ecosystem function and reveal the ecological processes that drive reef health and resilience.\nThrough these complementary analytical approaches, we transform field observations into ecological insights that inform conservation planning and contribute to our understanding of marine ecosystems across multiple scales.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#limitations",
    "href": "field-methods/benthic_inverts.html#limitations",
    "title": "Invertebrates",
    "section": "Limitations",
    "text": "Limitations\nWhile belt transects are an efficient method for surveying many mobile invertebrates, several limitations should be considered:\n\nHabitat complexity: Detection probability varies with substrate complexity\nCryptic behavior: Many invertebrates remain hidden within complex reef structures\nNocturnal activity: Daytime surveys miss species active primarily at night\nDiver avoidance: Some mobile species actively avoid observers\nSpatial clumping: Patchy distribution may result in high variance between transects\nTaxonomic challenges: Some groups require specialized expertise for accurate identification\n\nWe address these limitations through standardized protocols, thorough diver training, adequate replication, and integration with complementary methods like eDNA and night surveys where possible.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "field-methods/benthic_inverts.html#references",
    "href": "field-methods/benthic_inverts.html#references",
    "title": "Invertebrates",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Invertebrates"
    ]
  },
  {
    "objectID": "collaborative-infra/bigquery.html",
    "href": "collaborative-infra/bigquery.html",
    "title": "BigQuery",
    "section": "",
    "text": "The Pristine Seas Science Database is a centralized, modular system hosted in Google BigQuery that serves as the foundation of our data management infrastructure. This system integrates ecological data collected across more than a decade of scientific expeditions, supporting high-integrity, reproducible research on marine biodiversity and informing global ocean conservation policy.\n\n\nGoogle BigQuery offers several advantages for marine ecological data:\n\nScalability: Handles our growing dataset spanning 40+ expeditions without performance degradation\nIntegration: Seamlessly connects with our R-based analysis workflows\nCollaboration: Enables standardized access across our distributed research team\nFuture-proofing: Provides a robust platform that can evolve with our research needs\n\nNote: BigQuery is were Global Fishing Watch data lives and we have access to the raw backend data.\n\n\n\nOur database implementation adheres to the FAIR data principles:\n\n\n\n\n\nUnique and unified IDs\nWoRMS-linked taxonomy\nConsistent spatial hierarchy\n\n\n\n\n\nQuery-ready BigQuery tables\nControlled access and permissions\nVaried connection options\n\n\n\n\n\nSpatial reference system\nHarmonized taxonomic backbone\nConsistent units (cm, g, m²)\n\n\n\n\n\nComprehensive metadata\nReproducible code\nClear provenance6",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "BigQuery"
    ]
  },
  {
    "objectID": "collaborative-infra/bigquery.html#overview",
    "href": "collaborative-infra/bigquery.html#overview",
    "title": "BigQuery",
    "section": "",
    "text": "The Pristine Seas Science Database is a centralized, modular system hosted in Google BigQuery that serves as the foundation of our data management infrastructure. This system integrates ecological data collected across more than a decade of scientific expeditions, supporting high-integrity, reproducible research on marine biodiversity and informing global ocean conservation policy.\n\n\nGoogle BigQuery offers several advantages for marine ecological data:\n\nScalability: Handles our growing dataset spanning 40+ expeditions without performance degradation\nIntegration: Seamlessly connects with our R-based analysis workflows\nCollaboration: Enables standardized access across our distributed research team\nFuture-proofing: Provides a robust platform that can evolve with our research needs\n\nNote: BigQuery is were Global Fishing Watch data lives and we have access to the raw backend data.\n\n\n\nOur database implementation adheres to the FAIR data principles:\n\n\n\n\n\nUnique and unified IDs\nWoRMS-linked taxonomy\nConsistent spatial hierarchy\n\n\n\n\n\nQuery-ready BigQuery tables\nControlled access and permissions\nVaried connection options\n\n\n\n\n\nSpatial reference system\nHarmonized taxonomic backbone\nConsistent units (cm, g, m²)\n\n\n\n\n\nComprehensive metadata\nReproducible code\nClear provenance6",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "BigQuery"
    ]
  },
  {
    "objectID": "collaborative-infra/bigquery.html#database-architecture",
    "href": "collaborative-infra/bigquery.html#database-architecture",
    "title": "BigQuery",
    "section": "Database Architecture",
    "text": "Database Architecture\nThe Pristine Seas Science Database is organized around two major dataset groups that balance flexibility with consistency:\n\nMethod Datasets: Specific to each survey technique (UVS, BRUVS, eDNA, etc.)\nReference Datasets: Shared taxonomic, spatial, and lookup tables providing a unified backbone\n\n\nDatabase Organization\n\n\n\n\n\ngraph LR\n    %% Main project node\n    pristine[\"pristine-seas\"] \n    \n    %% First level - dataset categories\n    reference_group[\"Reference Datasets\"]\n    method_group[\"Method Datasets\"]\n    \n    %% Reference datasets\n    exp[\"expeditions/\"]\n    tax[\"taxonomy/\"]\n    look[\"lookup/\"]\n    \n    %% Method datasets\n    uvs[\"uvs/\"]\n    pbruv[\"pbruv/\"]\n    sbruv[\"sbruv/\"]\n    edna[\"edna/\"]\n    other[\"Other Methods...\"]\n       %% UVS tables\n    sites[\"sites\"]\n    stations[\"stations\"]\n    \n    %% Fish transect tables\n    blt_group[\"Fish Belt Transect Tables\"]\n    blt1[\"blt_stations\"]\n    blt2[\"blt_observations\"]\n    blt3[\"blt_biomass_by_taxa\"]\n    \n    %% LPI tables\n    lpi_group[\"Benthic LPI Tables\"]\n    lpi1[\"lpi_stations\"]\n    lpi2[\"lpi_counts\"]\n    lpi3[\"lpi_cover_by_taxa\"]\n    \n    %% Taxonomy tables\n    fish[\"fish\"]\n    benthos[\"benthos\"]\n    inverts[\"inverts\"]\n    \n    %% Expeditions tables\n    info[\"info\"]\n    exp_sites[\"sites\"]\n    \n    %% Connections\n    pristine --&gt; reference_group\n    pristine --&gt; method_group\n    \n    reference_group --&gt; exp\n    reference_group --&gt; tax\n    reference_group --&gt; look\n    \n    method_group --&gt; uvs\n    method_group --&gt; pbruv\n    method_group --&gt; sbruv\n    method_group --&gt; edna\n    method_group --&gt; other\n    \n    uvs --&gt; sites\n    uvs --&gt; stations\n    uvs --&gt; blt_group\n    uvs --&gt; lpi_group\n    \n    blt_group --&gt; blt1\n    blt_group --&gt; blt2\n    blt_group --&gt; blt3\n    \n    lpi_group --&gt; lpi1\n    lpi_group --&gt; lpi2\n    lpi_group --&gt; lpi3\n    \n    tax --&gt; fish\n    tax --&gt; benthos\n    tax --&gt; inverts\n    \n    exp --&gt; info\n    exp --&gt; exp_sites\n    \n    %% Styling with improved visibility and contrast\n    classDef root fill:#004165,color:#ffffff,stroke:#002e48,stroke-width:2px,rx:8,ry:8\n    classDef group fill:#f8f9fa,color:#000000,stroke:#343a40,stroke-width:1px,stroke-dasharray: 5 5,rx:5,ry:5\n    classDef refDataset fill:#d4edda,color:#000000,stroke:#28a745,stroke-width:2px,rx:8,ry:8\n    classDef methodDataset fill:#cce5ff,color:#000000,stroke:#0d6efd,stroke-width:2px,rx:8,ry:8\n    classDef table fill:#ffffff,color:#000000,stroke:#6c757d,stroke-width:1px,rx:3,ry:3\n    classDef tableGroup fill:#f8f9fa,color:#000000,stroke:#343a40,stroke-width:1px,rx:3,ry:3\n    \n    class pristine root\n    class reference_group,method_group group\n    class exp,tax,look refDataset\n    class uvs,pbruv,sbruv,edna,other methodDataset\n    class sites,stations,info,exp_sites,blt1,blt2,blt3,lpi1,lpi2,lpi3,fish,benthos,inverts table\n    class blt_group,lpi_group tableGroup\n\n\n\n\n\n\nFor the complete database documenation please refer to the Pristine Seas Database Documentation.\n\n\nCommon Dataset Structures\nEach method dataset follows a consistent pattern with tables organized into logical categories that support various analytical needs. This standardized structure enables efficient querying, cross-method integration, and reproducible science.\n\n\n\n\n\n\nStandard Table Types\n\n\n\n\nSite Tables (sites, [method]_sites)\n\nOne row per survey site or deployment\nContains spatial information, geographic coordinates, habitat descriptions, and context\nKey tables:\n\nexpeditions.sites: aggregates all survey sites across methods and expeditions over time\nuvs.sites: all underwater visual survey sites with method specific metadata.\nsub.sites: all sub dive sites with method specific metadata.\n\n\nStation Tables ([method]_stations)\n\nOne row per sampling unit (e.g., depth strata, replicate)\nIncludes spatial and temporal information, survey effort, and method-specific metadata\nInclude aggregated metrics of interest (e.g., fish biomass, % coral cover)\nFunctions as a primary unit for analysis and comparison\nKey tables:\n\nuvs.lpi_stations: all benthic LPI stations, including transect length, depth, habitat type, and summary metrics.\nuvs.blt_stations: all fish belt transect stations, including transect length, depth, habitat type, and summary metrics.\nsub.stations: all submersible survey stations (horizontal transects) done with the sub.\n\n\nObservation Tables ([method]_observations)\n\nContains individual records (e.g., fish counts, benthic points, video annotations)\nStores raw QAQC’d ecological data with validated taxonomic identifications\nKey tables:\n\nuvs.lpi_counts: all benthic LPI counts, including taxonomic identifications (field_name, morphotaxa, accepted_aphia_ID).\nuvs.blt_observations: all fish belt transect observations, including taxonomic identifications and estimated biomass.\n\n\nStation-Taxa Tables ([method]_[metric]_by_[dimension])\n\nAggregated analysis-ready metrics\nPre-calculated to standardize common analytical outputs\nEnables efficient cross-site and cross-method comparisons\nKey tables:\n\nuvs.cover_by_taxa: total point and % cover by morphotaxa and station.\nuvs.biomass_by_taxa: fish abundance and biomass by taxa and station.\npbruvs.Maxn_by_station: Max N and length estimates per taxa per station",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "BigQuery"
    ]
  },
  {
    "objectID": "collaborative-infra/bigquery.html#data-flow-field-to-database-pipeline",
    "href": "collaborative-infra/bigquery.html#data-flow-field-to-database-pipeline",
    "title": "BigQuery",
    "section": "Data Flow: Field to Database Pipeline",
    "text": "Data Flow: Field to Database Pipeline\nThe integration of expedition data into BigQuery follows a standardized workflow that ensures data quality and consistency:\n\n\n\n\n\nflowchart LR\n    subgraph Field[Field]\n        direction TB\n        A[Collection] --&gt; B[Validation]\n        B --&gt; C[Ship Storage]\n    end\n    \n    subgraph Process[Processing]\n        direction TB\n        D[Drive Backup] --&gt; E[Pipeline Processing]\n        E --&gt; F[Quality Control]\n    end\n    \n    subgraph Database[Database]\n        direction TB\n        G[BigQuery Ingestion] --&gt; H[Analysis-Ready Data]\n    end\n    \n    Field --&gt; Process\n    Process --&gt; Database\n    \n    classDef field fill:#004165,color:#ffffff,stroke-width:1px\n    classDef process fill:#8EBDC8,color:#000000,stroke-width:1px\n    classDef db fill:#E63946,color:#ffffff,stroke-width:1px\n    \n    class Field,A,B,C field\n    class Process,D,E,F process\n    class Database,G,H db\n\n\n\n\n\n\n\nProcess Steps\n\nField Collection: Researchers collect and record data using standardized methods and digital fieldbooks\nInitial Validation: Field-level data checks ensure completeness and quality\nNAS Storage: All expedition data is securely stored on the ship’s Network Attached Storage\nGoogle Drive Backup: Post-expedition, data is organized and backed up to Google Drive\nPipeline Processing: Each method’s data undergoes standardized processing via code in expedition repositories\nQuality Assurance: Automated and manual checks verify data integrity\nDatabase Ingestion: Processed data is ingested into the appropriate BigQuery tables\nAnalysis: Data becomes available for standardized analysis workflows",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "BigQuery"
    ]
  },
  {
    "objectID": "collaborative-infra/bigquery.html#access-and-use",
    "href": "collaborative-infra/bigquery.html#access-and-use",
    "title": "BigQuery",
    "section": "Access and Use",
    "text": "Access and Use\nThe Pristine Seas Science Database is designed to be accessible through multiple interfaces.\n\nRstudio\nThe most common way to interact with our BigQuery database is through R, using the familiar tidyverse workflow.\n\nEstablishing a Connection\nSetting up a connection is straightforward:\n\n# Load required packages\nlibrary(DBI)\nlibrary(bigrquery)\nlibrary(tidyverse)\n\n# Create a database connection\nbq_connection &lt;- DBI::dbConnect(bigrquery::bigquery(),\n                      project = \"pristine-seas\",\n                      billing = \"pristine-seas\")\n\nThis code establishes a connection to the entire database, allowing you to explore datasets and tables directly from the Connections pane in RStudio. The first time you run this code, you’ll be prompted to authenticate with your Google account.\n\n\nUsing dplyr Verbs\nThe real power comes from using familiar dplyr verbs directly with BigQuery tables—no SQL knowledge required:\n\ndf &lt;- tbl(bq_connection, \"taxonomy.fish\") |&gt; \n  group_by(family, genus) |&gt; \n  summarize(n_taxa = n_distinct(accepted_aphia_id),\n            .groups = \"drop\") |&gt; \n  arrange(desc(n_taxa)) |&gt; \n  head(30) |&gt; \n  collect()\n  \ndf |&gt; \n  ggplot()+\n  geom_col(aes(x = fct_reorder(family, n_taxa, sum), \n               y = n_taxa, \n               fill = genus), \n           show.legend = T)+\n  theme_minimal()+\n  coord_flip()+\n  labs(x = \"\", y = \"# Distinct Taxa\", fill = \"\",\n       title = \"Number of fish taxa in taxonomy.fish table\",\n       subtitle = \"By family and genus\")+\n  paletteer::scale_fill_paletteer_d(\"ggsci::default_igv\")\n\n\n\n\n\n\n\n\nWhat makes this approach powerful:\n\nBigQuery does the heavy lifting – All filtering, grouping, and summarizing happens in the database\nOnly results are transferred – Data never loads into memory until you call collect()\nFamiliar syntax – The same dplyr verbs you already use with local data frames\nReadable code – Complex queries expressed in clean, maintainable R code\n\n\n\n\nGoogle BigQuery Console\nThe database can also be accessed directly through the Google BigQuery Console:\n\nVisit console.cloud.google.com/bigquery\nNavigate to the pristine-seas project\nUse the query editor to write and execute SQL queries\nExplore tables, schemas, and query history\n\nThe console provides a user-friendly interface for exploring table structures, examining data samples, and running ad-hoc queries without writing code.\n\n\nAccess Management\nAccess to the Pristine Seas Science Database is managed through Google Cloud IAM:\n\nTeam Members: Full read access to all datasets\nCollaborators: Read access to specific datasets relevant to their work\nPartners: Access via shared exports or temporary read credentials\nPublic: Access to published, non-sensitive data via data packages (still TBD)\n\nTo request access, contact the database administrator with your Google account email and purpose.\n\n\nBest Practices\nWhen working with the Pristine Seas Science Database:\n\nMinimize data transfer: Filter data in BigQuery before collecting to R\nUse primary keys: Join tables using established keys (ps_station_id, ps_site_id)\nReproducible queries: Document your queries in Quarto documents\nAnalysis patterns: Build on established workflows in expedition repositories",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "BigQuery"
    ]
  },
  {
    "objectID": "collaborative-infra/bigquery.html#maintenance",
    "href": "collaborative-infra/bigquery.html#maintenance",
    "title": "BigQuery",
    "section": "Maintenance",
    "text": "Maintenance\nThe Pristine Seas Science Database is actively being developed and continously improved to meet the needs of our team.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "BigQuery"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html",
    "href": "collaborative-infra/argo_NAS.html",
    "title": "Argo NAS",
    "section": "",
    "text": "The Argo Network Attached Storage (NAS) serves as our primary data hub while at sea, providing high-capacity, RAID-configured local storage and file-sharing capabilities independent of internet connectivity. This system enables real-time collaboration, automated data backup, and efficient information exchange among all expedition members.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html#overview",
    "href": "collaborative-infra/argo_NAS.html#overview",
    "title": "Argo NAS",
    "section": "",
    "text": "The Argo Network Attached Storage (NAS) serves as our primary data hub while at sea, providing high-capacity, RAID-configured local storage and file-sharing capabilities independent of internet connectivity. This system enables real-time collaboration, automated data backup, and efficient information exchange among all expedition members.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html#system-architecture",
    "href": "collaborative-infra/argo_NAS.html#system-architecture",
    "title": "Argo NAS",
    "section": "System Architecture",
    "text": "System Architecture\nThe Argo NAS operates on a dedicated internal network accessible throughout the vessel, offering high-speed data transfer, automated synchronization, and robust storage redundancy. Each expedition member receives personalized access credentials with role-specific permission levels, ensuring appropriate access to data and collaboration spaces.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html#directory-structure",
    "href": "collaborative-infra/argo_NAS.html#directory-structure",
    "title": "Argo NAS",
    "section": "Directory Structure",
    "text": "Directory Structure\nFolders on the NAS mirror the structure of the matching expedition folder in Google Drive, ensuring a seamless transition between at-sea operations and post-expedition data management.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html#daily-workflow",
    "href": "collaborative-infra/argo_NAS.html#daily-workflow",
    "title": "Argo NAS",
    "section": "Daily Workflow",
    "text": "Daily Workflow\nThe NAS plays a central role in our daily expedition operations:\n\nData entry: Upload collected data and photos to the appropriate directories on the NAS (excluding remote camera footage)\nDaily backup: Automatic daily backup of all new uploads to ensure redundancy and data integrity\nEarly QAQC: Preliminary quality assurance and quality control (QA/QC) conducted by the data manager\nSpecies ID: Collaborative species identification and validation based on shared images and video\nPlanning: Preparation for the following day’s operations based on real-time sampling distribution updates\nHighlights: Sharing of daily highlights, including images, video clips, and key findings.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html#best-practices",
    "href": "collaborative-infra/argo_NAS.html#best-practices",
    "title": "Argo NAS",
    "section": "Best Practices",
    "text": "Best Practices\n\nFile Management\n\nFollow standard naming conventions for all folder and files:\n\nExample: benthos/site-photos/PLW-2023-uvs-001/..., sbruv/PLW_2023_sbruv_fieldbook.xlsx\n\nSave work to the NAS daily rather than keeping it on personal devices\nOrganize raw photos by station_id or date upon upload\n\n\n\nCollaborative Etiquette\n\nSave Frequently\nClose files when not in use to prevent access conflicts.\nShare your photos—contribute regularly to foster team collaboration",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html#support",
    "href": "collaborative-infra/argo_NAS.html#support",
    "title": "Argo NAS",
    "section": "Support",
    "text": "Support\nThe expedition’s data manager serves as the primary support contact for NAS-related issues, including troubleshooting, folder management, and QA/QC oversight.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/argo_NAS.html#post-expedition-data-management",
    "href": "collaborative-infra/argo_NAS.html#post-expedition-data-management",
    "title": "Argo NAS",
    "section": "Post-Expedition Data Management",
    "text": "Post-Expedition Data Management\nAt the conclusion of each expedition:\n\nAll data undergoes final validation and organization\nComplete dataset is backed up to portable drives\nThe full dataset (excluding raw footage) is synced to Google Drive.\nData is archived for future reference",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Argo NAS"
    ]
  },
  {
    "objectID": "collaborative-infra/zotero.html",
    "href": "collaborative-infra/zotero.html",
    "title": "Zotero",
    "section": "",
    "text": "The Pristine Seas Science Team maintains a centralized Zotero library for managing scientific literature. This shared resource ensures all team members have access to relevant publications, enabling consistent citation practices and facilitating collaborative research and writing. The library integrates with our broader collaborative infrastructure, maintaining parallel organizational structures with Google Drive and GitHub repositories.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Zotero"
    ]
  },
  {
    "objectID": "collaborative-infra/zotero.html#overview",
    "href": "collaborative-infra/zotero.html#overview",
    "title": "Zotero",
    "section": "",
    "text": "The Pristine Seas Science Team maintains a centralized Zotero library for managing scientific literature. This shared resource ensures all team members have access to relevant publications, enabling consistent citation practices and facilitating collaborative research and writing. The library integrates with our broader collaborative infrastructure, maintaining parallel organizational structures with Google Drive and GitHub repositories.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Zotero"
    ]
  },
  {
    "objectID": "collaborative-infra/zotero.html#library-structure",
    "href": "collaborative-infra/zotero.html#library-structure",
    "title": "Zotero",
    "section": "Library Structure",
    "text": "Library Structure\nOur Zotero library follows a hierarchical organization that balances expedition-specific needs with thematic categorization:\n\nBooks: Complete reference texts relevant to marine science and conservation\nEconomic Reports: Economic analyses and valuation studies of marine ecosystems\nExpedition References: Literature specific to expedition locations (organized by expedition ID)\n\nOne folder per expedition ID\nThematic subfolders within each expedition (fisheries, benthic communities, regional context)\n\nExpedition Reports: Archive of Pristine Seas expedition reports\nProjects: References for research projects\n\nUses consistent naming conventions that mirror Google Drive and GitHub structures\n\nPS Publications: Pristine Seas scientific publications organized by year\nGeneral References: Thematic collections of literature organized by scientific domain\n\nFisheries\nMPAs\nMesophotic reefs\nOther thematic categories",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Zotero"
    ]
  },
  {
    "objectID": "collaborative-infra/zotero.html#workflow-integration",
    "href": "collaborative-infra/zotero.html#workflow-integration",
    "title": "Zotero",
    "section": "Workflow Integration",
    "text": "Workflow Integration\n\nPre-Expedition\n\nScience lead establishes a new collection for the expedition using the standard expedition ID\nTeam contributes foundational literature about the expedition region including reports and documents from partners.\nScience lead exports and downloads key papers to Google Drive for sharing with external collaborators and the wider Pristine Seas team\nKey papers are flagged for required reading by expedition scientists\n\n\n\nDuring Expedition\n\nLibrary from Google Drive is loaded into the ship’s NAS for redundancy.\nReference materials support species identification and ecological interpretation\nZotero library remains accessible via laptops or tablets\n\n\n\nPost-Expedition Analysis and Reporting\n\nStandardized citation of references in reports and publications\nExport of formatted bibliographies for scientific reports\nAddition of new Pristine Seas publications to the library",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Zotero"
    ]
  },
  {
    "objectID": "collaborative-infra/zotero.html#best-practices",
    "href": "collaborative-infra/zotero.html#best-practices",
    "title": "Zotero",
    "section": "Best Practices",
    "text": "Best Practices\n\nOrganization\n\nFollow standardized naming conventions for collections\nEnsure all references include complete citation information\nAttach PDF files of publications when available\nConduct annual review and cleanup of library organization\n\n\n\nCitation Management\n\nUse of the APA 7th Edition citation style for consistency across all publications\nUtilization of the Google Docs Zotero plugin for collaborative manuscript writing\nDirect citation insertion in Quarto documents via Zotero integration with RStudio",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Zotero"
    ]
  },
  {
    "objectID": "collaborative-infra/github.html",
    "href": "collaborative-infra/github.html",
    "title": "GitHub",
    "section": "",
    "text": "GitHub serves as the foundation of our code management system, providing robust version control, collaborative workflows, and reproducible research capabilities. This central platform enables the Pristine Seas Science Team to maintain high standards of scientific rigor and transparency while facilitating efficient collaboration across our distributed team.\n\n\n\nVersion control: Track changes over time with complete history\nCollaboration: Multiple scientists can contribute to the same codebase\nReproducibility: Code tied to specific analyses or expeditions is preserved exactly as used\nKnowledge sharing: Public repositories make methods transparent to the broader scientific community\nQuality control: Pull request review system ensures code quality and consistency",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "GitHub"
    ]
  },
  {
    "objectID": "collaborative-infra/github.html#overview",
    "href": "collaborative-infra/github.html#overview",
    "title": "GitHub",
    "section": "",
    "text": "GitHub serves as the foundation of our code management system, providing robust version control, collaborative workflows, and reproducible research capabilities. This central platform enables the Pristine Seas Science Team to maintain high standards of scientific rigor and transparency while facilitating efficient collaboration across our distributed team.\n\n\n\nVersion control: Track changes over time with complete history\nCollaboration: Multiple scientists can contribute to the same codebase\nReproducibility: Code tied to specific analyses or expeditions is preserved exactly as used\nKnowledge sharing: Public repositories make methods transparent to the broader scientific community\nQuality control: Pull request review system ensures code quality and consistency",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "GitHub"
    ]
  },
  {
    "objectID": "collaborative-infra/github.html#organization-structure",
    "href": "collaborative-infra/github.html#organization-structure",
    "title": "GitHub",
    "section": "Organization Structure",
    "text": "Organization Structure\nThe Pristine Seas Science Team maintains a dedicated GitHub organization (pristine-seas) that houses all our code repositories, documentation, and analytical frameworks.\n\nRepository Types\nOur repositories fall into three main categories, each with specific naming conventions and purposes:\n\n\n\n\n\n\nExpedition Repositories\n\n\n\nNaming convention: exp-[ISO3]-[YEAR]\nPurpose: Code associated with an expedition, including data processing pipelines, analysis scripts, and report generation.\nExamples:\n\nexp-PNG-2024 (Papua New Guinea 2024)\nexp-COL-2022 (Colombia 2022)\nexp-GAB-2023 (Gabon 2023)\n\n\n\n\n\n\n\n\n\nProject Repositories\n\n\n\nNaming convention: prj-[Short-name]-[descriptor]\nPurpose: Code for research projects, thematic analyses, scientific papers, and curiosities.\nExamples:\n\nprj-Cyprus-trawling (Trawling impact analysis)\nprj-scandola-algae (Mediterranean algae study)\nprj-global-sharks (Global shark abundance patterns)\n\n\n\n\n\n\n\n\n\nCore Infrastructure Repositories\n\n\n\nNaming convention: Descriptive names\nPurpose: Central tools, packages, and resources.\nExamples:\n\nscience-sop (This Standard Operating Procedures documentation)\nlegacy-db (Legacy database build)\nPristineSeasR (Pristine Seas R package)\npristine-seas.github.io (Science Team website)",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "GitHub"
    ]
  },
  {
    "objectID": "collaborative-infra/github.html#repository-structure",
    "href": "collaborative-infra/github.html#repository-structure",
    "title": "GitHub",
    "section": "Repository Structure",
    "text": "Repository Structure\nExpedition repositories follow a standardized structure to ensure consistency and ease of navigation. This structure helps new team members quickly understand our workflow and facilitates efficient collaboration.\nexp-[ISO3]-[YEAR]/\n├── .github/              # GitHub specific files (actions, templates)\n├── R/                    # R functions and utility scripts\n├── pipeline/             # Data processing pipelines\n│   ├── pipe_benthic.qmd    # Benthic data processing\n│   ├── pipe_fish.qmd       # Fish data processing\n│   ├── pipe_pbruv.qmd      # BRUVS data processing\n├── eda/                  # Exploratory data analysis\n│   ├── eda_lpi.qmd       # Benthic data analysis\n│   ├── eda_fish.qmd      # Fish data analysis\n│   └── eda_pbruv.qmd     # Pelagic BRUVS data analysis\n├── docs/                 # qmds are rendered here\n├── .gitignore            # Files to exclude from version control\n├── README.md             # Repository overview and instructions\n└── exp-[ISO3]-[YEAR].Rproj  # RStudio project file\nProject and infrastructure repositories have flexible structures based on their specific needs, but should still maintain clear organization and documentation.\n\n\n\n\n\n\nData Storage Guidelines\n\n\n\nWe do not store data in GitHub repositories. This is both a practical consideration (GitHub has file size limits) and a scientific best practice (data should be managed separately from code).\nInstead:\n\nRaw data is stored in Google Drive (see Google Drive documentation)\nProcessed data is stored in BigQuery (see BigQuery documentation)",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "GitHub"
    ]
  },
  {
    "objectID": "collaborative-infra/github.html#github-workflow-with-r",
    "href": "collaborative-infra/github.html#github-workflow-with-r",
    "title": "GitHub",
    "section": "GitHub Workflow with R",
    "text": "GitHub Workflow with R\nOur team primarily interacts with GitHub through RStudio, following the principles outlined in Jenny Bryan’s “Happy Git with R” (happygitwithr.com). This approach simplifies version control while maintaining scientific rigor.\n\nBasic Git Workflow in RStudio\n\n\n\n\n\ngraph LR\n    A[Clone Repository] --&gt; B[Make Changes]\n    B --&gt; C[Stage Changes]\n    C --&gt; D[Commit Changes]\n    D --&gt; E[Pull]\n    E --&gt; F[Push]\n    style F fill:#d8f3dc,stroke:#95d5b2\n\n\n\n\n\n\nThe fundamental GitHub workflow for our team involves these key steps:\n\nClone: Create a local copy of the repository\n\nIn RStudio: File → New Project → Version Control → Git\nEnter repository URL: https://github.com/pristine-seas/repository-name.git\n\nMake Changes: Edit scripts, add analyses, or improve documentation\nStage: Select which changes to include in your commit\n\nIn RStudio: Use the Git pane to check boxes next to changed files\n\nCommit: Save your changes with a clear, descriptive message\n\nIn RStudio: Click “Commit” in the Git pane\nWrite a meaningful commit message that describes what you changed and why\n\nPull: Incorporate any changes others have made\n\nIn RStudio: Click “Pull” in the Git pane\nAddress any conflicts if they arise\n\nPush: Share your changes with the team\n\nIn RStudio: Click “Push” in the Git pane",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "GitHub"
    ]
  },
  {
    "objectID": "collaborative-infra/github.html#quarto-documents-for-reproducible-research",
    "href": "collaborative-infra/github.html#quarto-documents-for-reproducible-research",
    "title": "GitHub",
    "section": "Quarto Documents for Reproducible Research",
    "text": "Quarto Documents for Reproducible Research\nOur workflow heavily leverages Quarto documents (.qmd files) as the foundation for reproducible research. Quarto combines narrative text, code, and visualizations in a single document that serves as both lab notebook and reproducible pipeline.\n\nWhy We Use Quarto\nQuarto documents offer several advantages for scientific workflows:\n\nCode and narrative integration: Seamlessly mix text with executable R code\nReproducibility: Anyone can re-run analyses and get identical results\nSelf-documentation: Methods are explicitly documented alongside the code\nMultiple output formats: Generate HTML, PDF, Word, or presentations from the same source\nVersion control friendly: Text-based format works well with Git\n\n\n\nQuarto in Our Repository Structure\nOur expedition repositories are organized around two main types of Quarto documents:\n\nPipeline documents (pipeline/ folder):\n\nLinear processing workflows from raw data to analysis-ready datasets\nEmphasize data cleaning, validation, and transformation\nEach document focuses on a specific data stream (benthic, fish, etc.)\nFinal output is typically data tables ready for database ingestion\n\nExploratory Data Analysis (EDA) documents (eda/ folder):\n\nScientific analysis of processed data\nFocus on visualization, statistical testing, and interpretation\nOften include publication-quality figures\nCan be method-specific or integrate across multiple methods",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "GitHub"
    ]
  },
  {
    "objectID": "collaborative-infra/github.html#embracing-a-reproducibility-mindset",
    "href": "collaborative-infra/github.html#embracing-a-reproducibility-mindset",
    "title": "GitHub",
    "section": "Embracing a Reproducibility Mindset",
    "text": "Embracing a Reproducibility Mindset\nReproducibility is fundamental to scientific integrity and is a core value of the Pristine Seas Science Team. Beyond just technical practices, it requires a specific mindset when approaching our work.\n\nKey Principles\n\nThink of your future self: Write code and documentation as if you’ll need to understand it months from now with no memory of what you did\nThink of your teammates: Assume someone else will need to use and understand your work without having you available to explain it\nDocumentation is not optional: Clear documentation is as important as the code itself\n\nDocument the “why” not just the “how”\nExplain analytical choices and their implications\nNote data quirks and handling decisions\n\nNo magic numbers: Any constant or parameter should be explained and defined at the top of scripts\nEmbrace iteration: Start with messy exploration, but refine toward reproducible pipelines\n\nInitial EDA can be exploratory\nFinal analyses should be structured as reproducible workflows\nImprove code as clarity emerges\n\n\n\n\nPractical Habits\n\nStart each analysis session by pulling the latest code\nCommit frequently with clear messages\nReview your own work before considering it final\nTest with fresh environments to ensure all dependencies are documented\nKeep raw data pristine and document all transformations",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "GitHub"
    ]
  },
  {
    "objectID": "collaborative-infra/index.html",
    "href": "collaborative-infra/index.html",
    "title": "Overview",
    "section": "",
    "text": "Our collaborative infrastructure is designed to support expedition-based workflows, streamlining data collection, processing, and analysis across diverse environments. Each tool serves a specific role in our research and data lifecycle, from real-time field data entry to long-term archival and analysis. This interconnected system enables seamless collaboration, robust data management, and scientific rigor at every stage\n\n\n\n Google Drive\nCentralized storage hub for all expedition data, documents, and media.\nDetails →\n\n\n\n GitHub\nRepository for each expedition and project’s code and analysis.\nDetails →\n\n\n\n\n\n\n BigQuery\nCentralized database for integrated data over 10+ years of expeditions.\nDetails →\n\n\n\n Argo NAS\nAt-sea hub for collaborative work and file sharing.\nDetails →\n\n\n\n\n\n\n Zotero\nReference management for general and expedition-specific literature.\nDetails →",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Overview"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html",
    "href": "collaborative-infra/drive.html",
    "title": "Google Drive",
    "section": "",
    "text": "Google Drive serves as the primary collaborative hub for the Pristine Seas Science Team. As the foundation of our digital infrastructure, Drive provides centralized storage, enables collaboration, and ensures continuity of our scientific work.\n\nCentral Repository: Primary and secondary data from field to analysis\nDocument Management: Expedition plans, permits, reports, and manuscripts\nCollaboration Platform: Real-time collaborative editing and review\nAsset Library: Visual materials, presentations, and resources",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html#overview",
    "href": "collaborative-infra/drive.html#overview",
    "title": "Google Drive",
    "section": "",
    "text": "Google Drive serves as the primary collaborative hub for the Pristine Seas Science Team. As the foundation of our digital infrastructure, Drive provides centralized storage, enables collaboration, and ensures continuity of our scientific work.\n\nCentral Repository: Primary and secondary data from field to analysis\nDocument Management: Expedition plans, permits, reports, and manuscripts\nCollaboration Platform: Real-time collaborative editing and review\nAsset Library: Visual materials, presentations, and resources",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html#core-structure",
    "href": "collaborative-infra/drive.html#core-structure",
    "title": "Google Drive",
    "section": "Core Structure",
    "text": "Core Structure\nThe Pristine Seas Science Team Drive is organized into the following main folders:\nSCIENCE/\n├── expeditions/      # Primary repository for expedition data and documentation\n├── projects/         # Research projects and papers in support of our mission\n├── datasets/         # Useful and commoly used external datasets\n├── resources/          # Shared materials and assets\n│   ├── presentations/    # Slides for presentations, conferences, and meetings\n│   ├── media/            # Photos, videos for communications\n│   ├── illustrations/    # Scientific illustrations library\n│   ├── methods/          # Field protocols and standards\n│   ├── education/        # Training and outreach materials\n└── operations/       # Administrative content\n    ├── team/             # Team coordination\n    │   ├── meetings/        # Meeting notes and schedules\n    │   └── planning/        # Strategic documents\n    ├── equipment/        # Inventory and maintenance\n    ├── budgets/          # Financial planning",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html#expeditions",
    "href": "collaborative-infra/drive.html#expeditions",
    "title": "Google Drive",
    "section": "Expeditions",
    "text": "Expeditions\nThe expeditions folder is the heart of our data organization system, containing all materials related to our field expeditions. Expeditions are given standardized ID (exp_id) that’s composed of the ISO alpha-3 country code and the year of the expedition. In rare case when multiple expeditions occur in the same country within the same year, we add descriptive suffixes.\n\n\n\n\n\n\nExpedition ID (exp_id)\n\n\n\nFormat: [iso3]_[year]\nExamples: COL_2022, PLW_2023, ARG_2018_yaganes, ARG_2018_burwood\n\n\nThis naming system ensures expeditions are easily identifiable, correctly sorted by location and date, and consistently referenced across our platforms.\nexpeditions/\n└── iso3-year/\n    ├── readme.md                # Expedition overview\n    ├── data/                    # All expedition data\n    │   ├── primary/             # Data collected by team\n    │   │   ├── raw/             # Unmodified field data\n    │   │   ├── processed/       # QA/QC applied data\n    │   │   └── output/          # Analysis-ready data\n    │   └── secondary/           # Data from external sources\n    ├── documents/               # Scouting and planning documents\n    ├── figures/                 # Maps and visualizations\n    ├── gis/                     # Spatial project\n    ├── media/                   # Photos, videos\n    ├── presentations/           # Slide decks\n    ├── reports/                 # Expedition outputs\n    └── references/              # Literature and resources\n\n\n\n\n\n\nNaming conventions\n\n\n\nAll folder and file names should follow these conventions:\n\nFolders: Use kebab-case (hyphens), capitalize only proper nouns\n\nExamples: esv-PLW-2024/, prj-cyprus-trawlers/\n\nFiles: Use snake_case (underscores), capitalize only proper nouns\n\nExamples: FJI_2025_science_report.docx, YSI_manual.pdf\n\nNames should be descriptive and consistent\n\n\n\n\ndata/\nThe data folder is organized by source and processing stage:\n\nprimary/secondary/\n\n\nData collected by our team during the expedition:\n\nraw/:\n\nOriginal, unmodified fieldbooks\nSite and species photos\nHighlights from remote camera systems\n\nprocessed/:\n\nQA/QC procedures applied\nStandardized formats\n\noutput/:\n\nFully validated datasets\nDatabase-ready structure\nDerived metrics included\n\n\nEach subfolder is organized by research method (benthic, fish, bruvs, edna, etc.)\n\n\nData from external sources relevant to the expedition:\n\nprevious-work/: Previous research\nspatial/: Habitat maps, administrative boundaries\n\n\n\n\n\n\ndocuments/\nEssential expedition documentation:\n\nplanning/\n\nScouting reports and initial assessments\nScience plans detailing research objectives\n\npermits/\n\nResearch authorization applications and approvals\nMemoranda of understanding with local partners\n\nmisc/\n\nMeeting notes from planning and field sessions\nContact lists for team members and stakeholders\n\n\n\n\nmedia/\nExpedition visual documentation organized by source:\n\nscience-collection/\n\nSelect photos taken by science team members\nOrganized by species groups and habitats\n\nmedia-team/\n\nProfessional photography from our media team\nunderwater/: Underwater imagery of marine environments\ntopside: Aerial and landscape documentation\npeople: Human elements including team and communities\n\nfield-edit/\n\nExpedition video produced by the media team\n\n\n\n\nreports/\nExpedition outputs for different audiences:\n\nscience/\n\nComprehensive scientific report\nTwo-page expedition summary\n\noutreach/\n\nCommunity and stakeholder reports\nPress materials\nStories from the expedition\n\nassets/\n\nHigh-resolution images, maps and figures (.pdf)\nPartner logos or branding materials\n\n\n\n\nOther Key Folders\n\nfigures/\n\nMaps showing expedition sites and spatial patterns in data\nData visualizations from analyses\n\nGIS/\n\nStores the GIS project files\n\npresentations/\n\nPre-expedition planning briefings\nPost-expedition event materials\nOther stakeholder and partner presentations\n\nreferences/\n\nRegional scientific literature\nDomain-specific research papers\nCultural and historical context",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html#projects",
    "href": "collaborative-infra/drive.html#projects",
    "title": "Google Drive",
    "section": "Projects",
    "text": "Projects\nThe projects folder houses focused research initiatives and scientific papers that advance our mission. These may span global, regional, or expedition-specific contexts. Projects are given unique descriptive names that reflect their focus and objectives.\n\n\n\n\n\n\nProject ID\n\n\n\nEach project uses a standardized folder naming convention:\nFormat: prj-[descriptive]-[name]\nExamples: prj-SLI-resilience, prj-cachalote, prj-hawaii-lwr\n\n\n\nStructure\nA typical project structure contains the following folders:\nprojects/\n└── prj-[descriptive]-[name]/\n    ├── data/              # Project data (may link to but not copy expedition data)\n    ├── documents/         # Reference documents, meeting notes,\n    ├── figures/           # Plots, maps, visualizations\n    └── presentations/     # Project presentations\n\n\n\n\n\n\nGitHub Integration\n\n\n\n\nCode Management Policy\nAll code and scripts must be stored exclusively in GitHub repositories, never in Google Drive.\n\nDrive: Documents, data, media, and presentations\nGitHub: All code, scripts, and computational workflows\n\n\n\nRepository Naming Conventions\n\nExpeditions: exp-[iso3]-[year] (e.g., exp-MDV-2023)\nProjects: prj-[descriptive]-[name] (e.g., prj-cachalote)\n\nThis clear separation ensures proper version control, enables code review, facilitates collaboration, and maintains the integrity of our scientific workflow.",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html#datasets",
    "href": "collaborative-infra/drive.html#datasets",
    "title": "Google Drive",
    "section": "Datasets",
    "text": "Datasets\nThe datasets folder contains reference data from external sources that supports our analyses and provides essential context for our research. We maintain these datasets in their original form while providing comprehensive documentation.\n\nStructure\ndatasets/\n├── datasets_inventory.xlsx   # Master catalog with metadata and documentation\n├── aquamaps-v10-2019/        # Original dataset as obtained from source\n├── global-distribution-seagrass/\n├── gebco-2024/               # Bathymetry and ocean topography\n└── [dataset-name]/           # Additional reference datasets\n\n\nDataset Documentation\nOur datasets_inventory.xlsx catalogs each dataset with essential metadata:\n\nSource information: Original creator, publication references, access date\nGeographic scope: Spatial coverage, resolution, and coordinate systems\nTemporal range: Time period covered and update frequency\nContent description: Key variables, measurements, and units\nUsage limitations: Licensing, restrictions, and attribution requirements\n\n\n\nKey Reference Datasets\n\nmarine-regions: Standardized marine boundaries and maritime zones\nmpa-atlas: Global marine protected area database\ngravity-cinner-et-al: Market gravity model for human pressure\nhuman-impacts-on-ocean: Cumulative human impact assessments\nmarine-biogeographic-realms: Major biogeographic divisions\nmarine-ecoregions-and-provinces: Ecological classification system\nseamounts-yesson-2019: Global seamount locations and characteristics",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html#resources",
    "href": "collaborative-infra/drive.html#resources",
    "title": "Google Drive",
    "section": "Resources",
    "text": "Resources",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "collaborative-infra/drive.html#operations",
    "href": "collaborative-infra/drive.html#operations",
    "title": "Google Drive",
    "section": "Operations",
    "text": "Operations",
    "crumbs": [
      "Home",
      "Collaborative Infrastructure",
      "Google Drive"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pristine Seas Science Team SOP",
    "section": "",
    "text": "Living Document\n\n\n\nThis SOP is maintained collaboratively and updated regularly. Last updated: June 25, 2025."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Pristine Seas Science Team SOP",
    "section": "Introduction",
    "text": "Introduction\nWelcome to the Pristine Seas Science Team Standard Operating Procedures (SOP). This document serves as the comprehensive guide for conducting scientific research across Pristine Seas expeditions and projects. It establishes standardized protocols, workflows, and best practices to ensure consistency, reproducibility, and quality in all research activities.\n\nOur Mission\nPristine Seas works to explore, document, and protect vital places in the ocean. As the scientific foundation of this mission, our team conducts rigorous research to assess marine ecosystems and provide the evidence needed to support conservation efforts.\n\n\nCore Research Methods\nOur team uses a diverse set of integrated scientific methods to study marine ecosystems:\n\nBenthic surveys to quantify bottom types and invertebrates\nReef fish surveys to document fish communities\nEnvironmental DNA (eDNA) to assess overall biodiversity and to support the detection of cryptic and rare species\nBaited Remote Underwater Video Systems (BRUVS) to quantify predatory species\nSeabird and marine mammal surveys\nDeep water surveys using ROVs, submersibles, and drop-cameras\n\nExplore our methods →\n\n\nPurpose of this SOP\nThis SOP provides:\n\nCollaborative practices to enhance team efficiency\nStandardized protocols for data collection across expeditions\nData management workflows for organizing and storing data\nAnalytical frameworks for consistent data processing and analysis\n\nThe SOP ensures that our scientific practices meet the highest standards of rigor, transparency, and reproducibility. By following these guidelines, we produce reliable data that can inform conservation decisions and contribute to our understanding of marine ecosystems."
  },
  {
    "objectID": "index.html#core-principles",
    "href": "index.html#core-principles",
    "title": "Pristine Seas Science Team SOP",
    "section": "Core Principles",
    "text": "Core Principles\nThe Pristine Seas Science Team is guided by these fundamental principles in all our work:\n\n\n\nScientific Excellence\n\nCuriosity\nRigor\nPeer review\nImpact driven\n\n\n\n\nOpen Science\n\nReproducibilty\nTransparency\nFAIR data principles\n\n\n\n\nCollaborative Research\n\nInterdisciplinary approaches\nLocal scientist engagement\nBroad partner network"
  },
  {
    "objectID": "index.html#collaborative-framework",
    "href": "index.html#collaborative-framework",
    "title": "Pristine Seas Science Team SOP",
    "section": "Collaborative Framework",
    "text": "Collaborative Framework\nThe Pristine Seas Science Team works across an integrated set of platforms:\n\n\n\n\n\n\n\n GitHub\n\n\n\nVersion control for code, analysis, and documentation\n\n\n\n\n\n\n\n\n Google Drive\n\n\n\nCollaborative document editing and data storage\n\n\n\n\n\n\n\n\n Zotero\n\n\n\nReference management and bibliography\n\n\n\n\n\n\n\n\n\n\n BigQuery\n\n\n\nCentralized database for expedition data\n\n\n\n\n\n\n\n\n Argo NAS\n\n\n\nAt-sea hub for collaborative work\n\n\n\nLearn more about our collaborative tools →"
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Pristine Seas Science Team SOP",
    "section": "Get Started",
    "text": "Get Started\nReady to dive in? Explore these key sections:\n\n\nExpedition Planning\nPrepare for successful field research with comprehensive planning guidelines\n\n\nField Methods\nStandardized protocols for all Pristine Seas research methods\n\n\nData Science\nData workflows, R package documentation, and analysis standards\n\n\n\nMedia Library\nProtocols for media collection, organization, and metadata requirements"
  },
  {
    "objectID": "field-methods/ysi.html",
    "href": "field-methods/ysi.html",
    "title": "YSI",
    "section": "",
    "text": "Best method ever!\nSincerely, Turd",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Environmental (YSI)"
    ]
  },
  {
    "objectID": "field-methods/ysi.html#introduction",
    "href": "field-methods/ysi.html#introduction",
    "title": "YSI",
    "section": "",
    "text": "Best method ever!\nSincerely, Turd",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Environmental (YSI)"
    ]
  },
  {
    "objectID": "field-methods/ysi.html#method-overview",
    "href": "field-methods/ysi.html#method-overview",
    "title": "YSI",
    "section": "Method Overview",
    "text": "Method Overview",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Environmental (YSI)"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html",
    "href": "field-methods/benthic_recruits.html",
    "title": "Coral Recruits",
    "section": "",
    "text": "Coral recruitment is a fundamental process driving reef recovery, resilience, and long-term persistence. The Pristine Seas coral recruit survey methodology provides standardized protocols for quantifying juvenile coral abundance, taxonomic composition, and size distribution across different habitats and depths. These measurements serve as a key indicator of reef health, offering insights into recovery trajectories following disturbances and the potential for ecosystem renewal.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html#introduction",
    "href": "field-methods/benthic_recruits.html#introduction",
    "title": "Coral Recruits",
    "section": "",
    "text": "Coral recruitment is a fundamental process driving reef recovery, resilience, and long-term persistence. The Pristine Seas coral recruit survey methodology provides standardized protocols for quantifying juvenile coral abundance, taxonomic composition, and size distribution across different habitats and depths. These measurements serve as a key indicator of reef health, offering insights into recovery trajectories following disturbances and the potential for ecosystem renewal.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html#overview",
    "href": "field-methods/benthic_recruits.html#overview",
    "title": "Coral Recruits",
    "section": "Overview",
    "text": "Overview\nThe coral recruit survey method employs a systematic quadrat-based approach to document the abundance, taxonomic identity, and size structure of juvenile coral colonies. This method has been refined through years of field experience to balance thoroughness with field efficiency, providing quantitative data suitable for assessing reef recovery potential.\n\n\n\nDiver conducting coral recruit survey using a quadrat to systematically search for juvenile corals\n\n\n\nKey Features\n\nStandardized quadrats: Fixed-area sampling units (50 × 50 cm)\nSystematic placement: Regular intervals along established transects\nSize-based approach: Focus on colonies ≤5 cm maximum diameter\nTaxonomic resolution: Identification to genus or species level when possible\nPhotographic documentation: Each quadrat is photographed before examination\nIntegration: Same transect line used for LPI and invertebrate surveys\nSubstrate assessment: Documentation of settlement substrate types\nCondition evaluation: Notes on coral health, partial mortality, and bleaching",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html#field-implementation",
    "href": "field-methods/benthic_recruits.html#field-implementation",
    "title": "Coral Recruits",
    "section": "Field Implementation",
    "text": "Field Implementation\n\nTeam Structure\nThe coral recruit survey is conducted by t specialized diver as part of the three-person benthic survey team. The recruit/invertebrate specialist completes the coral recruit survey first, followed by the invertebrate survey along the same transect, optimizing dive time while maintaining thorough coverage.\n\n\n\n\n\n\nCoral Recruit Survey Specifications\n\n\n\n\nQuadrat dimensions: 50 × 50 cm (0.25 m²)\nQuadrat placement: Every 5 m along the transect (10 quadrats total)\nPlacement positions: 0, 5, 10, 15, 20, 25, 30, 35, 40, and 45 m marks\nTotal survey area: 2.5 m² per transect\nDepth strata: Typically 10 m and 20 m (matching other benthic methods)\nTarget size range: Corals ≤5 cm maximum diameter\nTiming: Conducted before invertebrate surveys\n\n\n\n\n\nCollection Procedure\n\nDeployment coordination:\n\nThe recruit survey utilizes the same 50 m transect line deployed for the LPI survey\nBegin at the 45 m mark and proceed along the transect towards the start.\n\nQuadrat placement:\n\nPlace the quadrat on the substrate at each 5 m interval along the transect (0, 5, 10, …, 40, and 45 m marks)\nEnsure the quadrat is properly positioned without disturbing small corals\nThe combined survey area across all quadrats is 2.5 m² per transect\n\nQuadrat documentation:\n\nBefore searching for recruits, photograph each quadrat from above\nInclude the quadrat ID tag in the image for reference\nThese images serve as verification and provide context for identified recruits\n\nQuadrat examination:\n\nSystematically search the entire quadrat area in a consistent pattern\nFocus on colonies ≤5 cm maximum diameter with distinct tissue and skeletal boundaries\nUse the edge of dive slate marked in cm to measure sizes\nExclude fragments or remnants of larger colonies\n\nData recording:\n\nFor each juvenile coral within the quadrat:\n\nIdentify to lowest possible taxonomic level (genus or species)\nMeasure maximum diameter to the nearest 0.5 mm (for colonies larger than 0.5 cm)\nNote the quadrat number to link records back to the quadrat image\nDocument attachment substrate type\nNote any partial mortality, bleaching, or disease\n\nRecord quadrat-level information:\n\nDominant substrate type\nApproximate sediment cover (%)\n\n\nDetailed documentation:\n\nPhotograph unidentified or unusual recruits for later verification\nTake reference images of representative recruits\nCapture close-up images of taxonomically challenging specimens\n\nSurvey completion:\n\nAfter examining all 10 quadrats, stow equipment securely\nTransition to invertebrate surveys (working back along the same transect)\nNote any unusual observations or methodological deviations",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html#taxonomic-framework",
    "href": "field-methods/benthic_recruits.html#taxonomic-framework",
    "title": "Coral Recruits",
    "section": "Taxonomic Framework",
    "text": "Taxonomic Framework\nAccurate identification of juvenile corals presents unique challenges due to their small size and undeveloped morphological features. Pristine Seas employs a comprehensive taxonomic system that balances field practicality with scientific rigor, with the World Register of Marine Species (WoRMS) AphiaID serving as the authoritative identifier throughout all datasets.\n\nIdentification Process\nOur taxonomic approach adapts to the inherent challenges of identifying juvenile corals in the field. The most significant difficulty stems from the dramatic morphological differences between juvenile and adult coral colonies:\n\nUndeveloped morphology: Young colonies lack the characteristic growth forms that define adult corals\nLimited polyp development: Colonies often have only a few polyps, obscuring diagnostic features\nCryptic placement: Recruits frequently settle in concealed locations, complicating thorough examination\nDevelopmental plasticity: Early growth forms may differ substantially from adult morphology\n\nTo address these challenges, we implement a size-based hierarchical approach that matches identification precision to observable features:\n\n\n\n\n\n\nSize-Based Identification Approach\n\n\n\nWe implement a hierarchical approach that matches identification precision to colony size and observable features:\nFor colonies smaller than 1 cm: Focus on basic skeletal structures and polyp arrangements, typically achieving family-level identification and sometimes genus-level when distinctive features are present.\nFor colonies between 1-3 cm: Incorporate tissue characteristics and emerging morphology, reliably achieving genus-level identification in most cases. Color patterns, polyp arrangement, and initial branching patterns become more distinctive.\nFor colonies between 3-5 cm: Apply standard adult coral identification criteria where applicable, enabling genus-level identification in nearly all cases and species-level identification when key diagnostic features are present.\n\n\nThroughout this process, photographic documentation supports field identifications, allowing for post-dive verification and consultation with coral taxonomy specialists when necessary. This systematic approach acknowledges the practical limitations of field identification while maximizing taxonomic precision at each developmental stage.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html#data-workflow",
    "href": "field-methods/benthic_recruits.html#data-workflow",
    "title": "Coral Recruits",
    "section": "Data Workflow",
    "text": "Data Workflow\n\nData Entry\nCoral recruit data are recorded in standardized Excel fieldbooks on the same day of data collection, while information and memory are fresh. The ISO3_YEAR_recruits_fieldbook.xlsx is organized as follows:\n\nreadme sheet: Contains expedition info, data entry instructions, and guidelines\nstations sheet: Records station information including sampling depths, habitat types, and any deviations from protocol\nrecruits sheet: Primary data entry for all coral recruits, including , size measurements and substrate type, and quadrat level info\n\nThe recruits sheet includes validation rules that flag potential errors such as:\n\nSize measurements exceeding the 5 cm maximum threshold\nMissing taxonomic information\nInvalid quadrat numbers\n\n\n\n\n\n\n\nQuadrat Image Organization\n\n\n\nThe systematic organization of quadrat images is essential for later reference and quality control:\n\nNaming convention: [ISO3]_[YEAR]_recruits_[SITE_ID]_[DEPTH]_Q[##].jpg\nExample: PLW_2023_recruits_005_10m_Q03.jpg (Palau, 2023, site 5, 10m depth, quadrat 3)\nStorage structure: Images stored in folders mirroring site → station hierarchy\nBackup protocol: Daily transfer to expedition NAS and cloud backup when possible\n\n\n\n\n\nProcessing Pipeline\nOur data processing transforms raw field observations into analysis-ready datasets through:\n\n1. File Consolidation\nFirst, we compile data from all divers and expedition legs into a unified dataset:\n\nMerge all recruits records from multiple dives and sites\nStandardize taxonomic names using our coral taxonomy reference table\nAdd metadata from the UVS Sites Fieldbook (coordinates, habitat type, etc.)\nValidate image references and ensure all quadrats are properly documented\nCreate a complete dataset that preserves all original observations\n\n\n\n2. Quality Control\nNext, we verify data quality through a systematic review process:\n\nAnalyze size distributions to identify unlikely measurements\nCompare taxonomic identifications between observers for consistency\nCross-check recruit taxa against adult corals identified in LPI surveys\nVerify substrate categorizations for consistency\nEnsure complete sampling coverage (all 10 quadrats per transect)\n\nPotential issues are flagged for expert review rather than automatically removed, preserving data integrity while ensuring scientific accuracy.\n\n\n3. Density Calculation\nWe then calculate standardized density metrics for cross-site comparisons:\n\nCompute recruit density (colonies/m²) by taxa and size class\nAccount for the actual area sampled when fewer than 10 quadrats were completed\nGenerate density metrics at multiple taxonomic levels (species, genus, family)\nCalculate size-based metrics (mean diameter, size class frequencies)\n\nA critical step in this process is the addition of zero values for taxa that were not observed in a particular quadrat but were found elsewhere at the site. This ensures that absence data is properly incorporated when calculating average densities and prevents biased estimates. Zero values are applied:\n\nAfter taxonomic standardization but before aggregation\nBased on the observed taxa pool within each site\nOnly for taxa recorded in at least one quadrat at the site\nPrior to calculating transect-level and station-level metrics\n\n\n\n4. Database Integration\nFinally, we format the processed data for ingestion into our central database, creating four core output files:\n\nrecruits_observations: Individual coral records with standardized taxonomy and measurements\nrecruits_density_by_taxa: Density calculations by taxa, size class, and station\nrecruits_station_summary: Aggregate metrics by station (total density, diversity indices)\nrecruits_taxa_summary: Region-wide patterns of occurrence and average densities\n\nThese standardized outputs allow consistent analysis across expeditions and regions while maintaining connections to the original field observations.\n\n\n\nExploratory Data Analysis\nOur analytical approach examines coral recruitment through multiple complementary perspectives:\n\nRecruitment Patterns\nWe analyze recruitment density and taxonomic composition across sites and regions:\n\nTotal recruit density comparisons across habitat types and other gradients\nTaxonomic composition analysis to identify dominant recruiting taxa\nComparison with regional and global benchmarks\nCorrelation with adult coral cover and community structure from LPI surveys\n\nThese analyses provide insights into the factors controlling recruitment success and how it relates to broader ecological patterns.\n\n\nSize Structure Assessment\nSize distributions provide insights into recruitment timing and post-settlement survival:\n\nSize frequency analysis by taxa and life history strategy\nCohort identification to detect recruitment pulses\nComparison of size structures across environmental gradients\nIntegration with known disturbance history\n\nSize structure analysis is particularly valuable for identifying recruitment timing and evaluating post-settlement survival patterns.\n\n\nSubstrate Relationships\nWe examine how settlement substrate influences recruitment success:\n\nQuantification of substrate preferences by coral taxa\nRelationship between substrate availability and recruitment density\nIdentification of recruitment microhabitats\nRole of crustose coralline algae (CCA) in facilitating settlement\n\nThese patterns reveal important information about habitat requirements for successful coral recruitment.\n\n\nSpatial Patterns\nBy analyzing recruitment patterns across space, we identify:\n\nRecruitment hotspots and priority conservation areas\nEnvironmental correlates of recruitment success\nDepth-related patterns in recruit abundance and composition\nConnectivity implications for population replenishment\n\n\n\nCross-Method Integration\nWe combine recruit data with other survey components to reveal ecosystem relationships:\n\nCorrelations between coral cover and recruit density\nRelationships between herbivorous fish and recruitment success\nConnections between benthic composition and recruit diversity\nPotential feedback loops linking predators, grazers, and coral recruitment\n\nThese integrated analyses provide a holistic understanding of reef ecosystem function and reveal the ecological processes that drive reef recovery and resilience.\nThrough these complementary analytical approaches, we transform field observations into ecological insights that inform conservation planning and contribute to our understanding of marine ecosystems—from individual reefs to global patterns.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html#limitations",
    "href": "field-methods/benthic_recruits.html#limitations",
    "title": "Coral Recruits",
    "section": "Limitations",
    "text": "Limitations\nWhile quadrat surveys are an effective method for assessing coral recruitment, several limitations should be considered:\n\nDetection challenges: The smallest recruits (&lt;5 mm) are extremely difficult to detect in field conditions, potentially leading to underestimates of recent recruitment\nTaxonomic uncertainty: Accurate species-level identification of recruits is challenging and often not possible for the smallest size classes\nSpatial heterogeneity: Recruitment can be highly patchy, requiring sufficient sample sizes to characterize site-level patterns\nTemporal variability: Recruitment is often episodic, with significant seasonal and annual variations that may not be captured in single surveys\nPost-settlement mortality: High mortality rates in the first weeks after settlement mean that visible recruits represent only a small fraction of initial settlers\nMicrohabitat bias: Recruits in cryptic microhabitats (e.g., crevices, undersides) may be undersampled\n\nWe address these limitations through standardized protocols, thorough diver training, adequate replication, and integration with complementary methods like adult coral surveys and eDNA.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_recruits.html#references",
    "href": "field-methods/benthic_recruits.html#references",
    "title": "Coral Recruits",
    "section": "References",
    "text": "References\nDarling ES, Alvarez-Filip L, Oliver TA, McClanahan TR, Côté IM (2012) Evaluating life-history strategies of reef corals from species traits. Ecology Letters 15(12): 1378-1386. https://doi.org/10.1111/j.1461-0248.2012.01861.x\nEdmunds PJ, Leichter JJ, Johnston EC, Tong EJ, Toonen RJ (2016) Ecological and genetic variation in reef-building corals on four Society Islands. Limnology and Oceanography 61(2): 543-557. https://doi.org/10.1002/lno.10231\nRitson-Williams R, Arnold SN, Fogarty ND, Steneck RS, Vermeij MJ, Paul VJ (2009) New perspectives on ecological mechanisms affecting coral recruitment on reefs. Smithsonian Contributions to the Marine Sciences 38: 437-457.\nSommer B, Harrison PL, Beger M, Pandolfi JM (2014) Trait-mediated environmental filtering drives assembly at biogeographic transition zones. Ecology 95(4): 1000-1009. https://doi.org/10.1890/13-1445.1",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Coral recruits"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html",
    "href": "field-methods/benthic_lpi.html",
    "title": "Line Point Intercept",
    "section": "",
    "text": "The Line Point Intercept (LPI) method is a fundamental component of Pristine Seas underwater visual surveys, providing quantitative data on benthic community composition and substrate cover. This method documents the relative abundance of key benthic categories, including corals, algae, and other sessile organisms, as well as abiotic substrate types.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#introduction",
    "href": "field-methods/benthic_lpi.html#introduction",
    "title": "Line Point Intercept",
    "section": "",
    "text": "The Line Point Intercept (LPI) method is a fundamental component of Pristine Seas underwater visual surveys, providing quantitative data on benthic community composition and substrate cover. This method documents the relative abundance of key benthic categories, including corals, algae, and other sessile organisms, as well as abiotic substrate types.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#method-overview",
    "href": "field-methods/benthic_lpi.html#method-overview",
    "title": "Line Point Intercept",
    "section": "Method Overview",
    "text": "Method Overview\nThe LPI method involves recording the benthic organisms or substrate types that occur directly beneath points at regular intervals along a transect line. This approach provides an unbiased estimate of percent cover for different benthic categories.\n\nKey Features\n\nStandardized points: Data collected at fixed intervals (20 cm)\nRepresentative sampling: Points distributed evenly along the entire transect\nObjective assessment: Only what lies directly under each point is recorded\nTaxonomic resolution: Identification to the lowest practical taxonomic level\nDivision of labor: Separate divers for coral and other benthic identifications",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#field-implementation",
    "href": "field-methods/benthic_lpi.html#field-implementation",
    "title": "Line Point Intercept",
    "section": "Field Implementation",
    "text": "Field Implementation\nThe LPI method requires two specialized divers working in tandem:\n\nBenthic diver: Identifies all algae, non-coral invertebrates, and abiotic substrates at each point. When a point lands on coral, the benthic diver marks it as “Hard coral” (or “Hard coral - bleached”) without species identification.\nCoral diver: Focuses specifically on identifying all corals to species level at points previously marked as “Hard coral” by the benthic diver.\n\nThis division of labor ensures both broad coverage of all substrate types and precise taxonomic resolution for corals.\n\n\n\n\n\n\nLPI Transect Specifications\n\n\n\n\nLength: 50 meters (Note: transects may be shorter due to time or decompression constraints)\nDirection: Parallel to shoreline\nDepth strata: Constant (±1 m) standard depth strata (~5, 10 or 20 m)\nPoints: Sampled every 20 cm (250 points total)\nSections: Transects are divided into sections (5 of 10 m each)\n\n\n\n\nData Collection Procedure\n\nTransect deployment:\n\nDeploy transect tape parallel to shoreline\nMaintain consistent depth throughout\nSecure ends to maintain straight line\n\nPoint sampling:\n\nSample at 20 cm intervals along the transect (250 points total)\nRecord what lies directly beneath each point\nBenthic diver identifies all non-coral points to lowest taxonomic level\nCoral diver identifies coral points to species level\n\nBleaching assessment:\n\nFor hard corals: differentiate between healthy, bleaching, and dead colonies\nFor soft corals: distinguish bleached and non-bleached colonies\n\nPhoto documentation:\n\nTake overview photo of transect\nPhotograph representative points\nDocument unusual or unidentified organisms\n\nExplore:\n\nRoam the station if time allows\nDocument any notable off-transect taxa or observations",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#taxonomic-framework",
    "href": "field-methods/benthic_lpi.html#taxonomic-framework",
    "title": "Line Point Intercept",
    "section": "Taxonomic Framework",
    "text": "Taxonomic Framework\nThe Pristine Seas LPI method employs world-class scientific divers who typically identify organisms to species or genus level immediately. However, for challenging identifications, we use a structured taxonomic resolution model that ensures scientific rigor throughout the identification process.\n\n\nCode\ngraph TD\n    classDef fieldName fill:#F8F9FA,stroke:#DEE2E6,stroke-width:2px,color:#000000,font-size:14px\n    classDef morphotaxon fill:#E9EDF9,stroke:#6C8EBF,stroke-width:2px,color:#000000,font-size:14px\n    classDef minTaxon fill:#D4E4F9,stroke:#3F75BF,stroke-width:2px,color:#000000,font-size:14px\n    classDef funcCat fill:#004165,stroke:#002E48,stroke-width:2px,color:#FFFFFF,font-size:14px\n    \n    A[Field Name&lt;br&gt;'blue branching coral'] --&gt; B[Morphotaxon&lt;br&gt;'Acropora sp. blue']\n    B --&gt; C[Lowest Defensible Taxonomic Rank&lt;br&gt;'Acropora sp.']\n    C --&gt; D[Benthic Functional Category&lt;br&gt;'Hard Coral']\n    \n    class A fieldName\n    class B morphotaxon\n    class C minTaxon\n    class D funcCat\n    \n    style D fill:#EA9E8D,stroke:#D87F6A,stroke-width:2px,color:#000000\n\n\n\n\n\ngraph TD\n    classDef fieldName fill:#F8F9FA,stroke:#DEE2E6,stroke-width:2px,color:#000000,font-size:14px\n    classDef morphotaxon fill:#E9EDF9,stroke:#6C8EBF,stroke-width:2px,color:#000000,font-size:14px\n    classDef minTaxon fill:#D4E4F9,stroke:#3F75BF,stroke-width:2px,color:#000000,font-size:14px\n    classDef funcCat fill:#004165,stroke:#002E48,stroke-width:2px,color:#FFFFFF,font-size:14px\n    \n    A[Field Name&lt;br&gt;'blue branching coral'] --&gt; B[Morphotaxon&lt;br&gt;'Acropora sp. blue']\n    B --&gt; C[Lowest Defensible Taxonomic Rank&lt;br&gt;'Acropora sp.']\n    C --&gt; D[Benthic Functional Category&lt;br&gt;'Hard Coral']\n    \n    class A fieldName\n    class B morphotaxon\n    class C minTaxon\n    class D funcCat\n    \n    style D fill:#EA9E8D,stroke:#D87F6A,stroke-width:2px,color:#000000\n\n\n\n\n\n\nOur taxonomic resolution progresses through four distinct levels:\n\nField Names: When species-level identification isn’t immediately possible, divers use descriptive temporary placeholders based on distinguishing characteristics.\nExample: “blue branching coral,” “tall green algae,” “orange encrusting sponge”\nMorphotaxa: Field names are consolidated into taxonomic units that combine formal taxonomy with distinguishing field characteristics. This process begins onboard the expedition vessel and evolves as divers gain familiarity with local taxa.\nExample: “Acropora sp. blue,” “Halimeda sp. tall,” “Clathria sp. orange”\nLowest Defensible Taxonomic Rank: Morphotaxa are mapped to the lowest taxonomic level that can be scientifically defended based on available evidence.\nExample: “Acropora sp.,” “Halimeda sp.,” “Porifera”\nBenthic Functional Categories: Taxonomic ranks are classified into ecological functional groups for ecosystem-level analyses. These are derived from standardized taxonomy reference lists.\nExample: “Hard Coral,” “Erect Algae,” “Sponges”\n\n\n\n\n\n\n\nTaxonomic Identification Workflow\n\n\n\nA Progressive Workflow\n\nDuring dives\n\nDivers identify organisms to species level when possible\nFor challenging taxa, descriptive field names are used as placeholders\n\nOnboard processing\n\nConsolidation into morphotaxa begins immediately onboard:\n\nDivers review photographs and compare observations\nTaxonomic knowledge improves progressively throughout expedition\nField guides and reference materials guide identifications\n\n\nPost-expedition refinement\n\nFinal taxonomic processing using expedition reference taxonomy lists:\n\nMorphotaxa → Lowest defensible taxonomic rank\nTaxonomic rank → Functional group\nFor corals: Addition of morphology and life history classifications",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#ecological-classifications",
    "href": "field-methods/benthic_lpi.html#ecological-classifications",
    "title": "Line Point Intercept",
    "section": "Ecological Classifications",
    "text": "Ecological Classifications\nWe use the following standardized functional categories for analysis and visualization:\n\n\n\nHard Coral Scleractinian corals with calcium carbonate skeletons Examples: Acropora spp., Porites spp.\n\n\n\nCCA (Crustose Coralline Algae) Pink to red calcified encrusting algae Examples: Porolithon spp., Lithophyllum spp., Hydrolithon spp.\n\n\n\nCyanobacteria Photosynthetic bacterial mats Examples: Lyngbya spp., Oscillatoria spp., Symploca spp.\n\n\n\nSoft Coral Non-reef building octocorals Examples: Sinularia spp., Sarcophyton spp., Lobophytum spp.\n\n\n\nSponges Porifera of various morphologies Examples: Xestospongia spp., Haliclona spp., Cliona spp.\n\n\n\nEncrusting Algae Non-coralline encrusting macroalgae Examples: Peyssonnelia spp., Lobophora spp., Ralfsia spp.\n\n\n\nErect Algae Upright macroalgae with distinct structure Examples: Sargassum spp., Turbinaria spp., Halimeda spp.\n\n\n\nTurf Low-growing filamentous algal assemblages Examples: Mixed filamentous turf, cyanobacterial turf, red turf\n\n\n\nOther Organisms that don’t fit main categories Examples: Ascidians, bryozoans, hydroids\n\n\n\nSediment/Rubble/Barren Abiotic substrates and bare surfaces Examples: Sand, coral rubble, bare rock\n\n\n\n\nHard Corals\nHard corals are classified using complementary systems that enable comprehensive analysis of community structure, ecological function, and resilience potential across survey sites.\n\nLife History Strategies\nCoral species exhibit distinct life history strategies that reflect evolutionary adaptations to different environmental conditions. We map our coral taxa to the four strategies proposed by Darling et al. (2012) primarily distinguished by colony morphology, growth rate, and reproductive mode. These strategies help predict species responses to disturbances and inform conservation approaches in the rapidly changing marine environment.\n\nCompetitiveWeedyStress-TolerantGeneralist\n\n\nTraits\n\nFast growth, broadcast spawning, large colonies with low skeletal density.\nDominate favorable environments and outcompete others for space.\n\nDisturbance tolerance: Vulnerable to bleaching and physical damage with poor recovery capacity.\nExamples: Acropora palmata, A. cervicornis, Pocillopora elegans\nMorphologies: Branching, plating, corymbose forms creating complex habitats.\n\n\nTraits:\n\nVariable growth, brooding reproduction, high reproductive output, small colonies (&lt;20cm).\nPioneer species colonizing disturbed areas.\n\nDisturbance tolerance: Moderate bleaching susceptibility but quick recovery through rapid recruitment and effective dispersal.\nExamples: Pocillopora damicornis, Stylophora pistillata, Seriatopora hystrix\nMorphologies: Small branching, digitate forms establishing rapidly on available substrate.\n\n\nTraits:\n\nSlow growth (&lt;5 cm/year), broadcast spawning, conservative energy use, high skeletal density.\nLong-lived colonies persisting in marginal environments.\n\nDisturbance tolerance: Resistant to acute stressors, withstand higher temperatures, slow but persistent recovery.\nExamples: Porites lobata, Orbicella annularis, Siderastrea siderea\nMorphologies: Massive, boulder, encrusting forms providing reef framework stability.\n\n\nTraits:\n\nModerate growth (5-15 cm/year), mixed reproduction strategies, balanced resource allocation.\nAdaptable across environmental gradients.\n\nDisturbance tolerance: Intermediate thermal tolerance with moderate recovery capacity and variable stress responses.\nExamples: Diploria labyrinthiformis, Pavona spp., Galaxea fascicularis\nMorphologies: Massive, foliose, meandroid forms with moderate complexity.\n\n\n\n\n\nMorphology\nCoral morphology represents the physical structure and growth form that coral colonies develop, directly influencing their ecological function and response to environmental stressors. These distinct growth forms are key determinants in coral life history strategies and provide important insights into reef structural complexity, habitat provision, and resilience potential.\n\n\nBranching\n\nTree-like with thin branchese.g., Acropora muricata\n\n\n\nTabular\n\nHorizontal plate-like growthe.g., Acropora hyacinthus\n\n\n\nMassive\n\nBoulder-like, robust growthe.g., Porites lobata\n\n\n\nEncrusting\n\nGrowing flat against substratee.g., Montipora spumosa\n\n\n\nFoliose\n\nLeaf-like or whorl structuree.g., Turbinaria reniformis\n\n\n\nSolitary\n\nIndividual polypse.g., Fungia fungites\n\n\n\nDigitate\n\nFinger-like branchese.g., Pocillopora meandrina\n\n\n\nCorymbose\n\nTable-like with branchletse.g., Acropora cytherea",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#data-workflow",
    "href": "field-methods/benthic_lpi.html#data-workflow",
    "title": "Line Point Intercept",
    "section": "Data Workflow",
    "text": "Data Workflow\n\nData Entry\nLPI data are recorded in standardized Excel fieldbooks on the same day of data collection, while information and memory are fresh. The ISO3_YEAR_lpi_fieldbook.xlsx is organized as follows:\n\nreadme sheet: Contains expedition info, data entry instructions, and guidelines\nstations sheet: Records station information including sampling depths, survey lengths, and any deviations from protocol.\nobservations sheet: Primary data entry for all LPI point intercepts\n\n\n\nProcessing Pipeline\n\nFile Consolidation\nThe workflow begins by reading all expedition fieldbooks across legs and divers, combining stations and observations into a unified dataset. This includes converting the data from a wide format (best for data entry in the field) to a long data format, best for anaysis and visualiztion. It also include merginbg the work of the benthic and coral divers.\n\n\n\n\n\n\nMerging Benthic and Coral Diver Data\n\n\n\nThis critical step integrates the specialized knowledge of both divers:\n\nBenthic diver identifies functional groups (e.g., “30 points as hard coral”)\nCoral diver provides detailed taxonomic identification of corals\nWe apportion the benthic diver’s points based on the coral diver’s proportional observations\nThis maintains accurate functional group counts while maximizing taxonomic resolution\n\n\n\n\n\nValidation and Cleaning\nStation data is joined with the uvs_sites_fieldbook to validate site information including coordinates, habitat type, and spatial hierarchy. We check depth strata alignment across stations, standardizes formats, and verifies completeness of required fields. Transects are validated to confirm they contain the expected number of points, with exceptions flagged for review.\nAll taxonomic entries undergo validation against master reference lists, with morphotaxa mapped to the appropriate classification hierarchy.\n\n\nBenthic Cover Calculation\nFrom the cleaned data, we calculate percent cover by morphotaxa and functional group for each station. Additional metrics include species richness, diversity indices, and coral health indicators such as bleaching percentages. These station-level calculations are then aggregated to create site averages and region-wide metrics for comparative analysis.\n\n\n\n\n\n\nCore Output Files\n\n\n\nThe pipeline generates four essential files:\n\nlpi_count: raw but clean raw point contacts\nlpi_cover_by_taxa: total points and % cover by morphotaxa and station\nlpi_station_summary: % cover by functional category and station\nlpi_taxa_summary.csv: prevalence across regions, frequency of occurrence, and average cover values\n\n\n\n\n\nDatabase Integration\nFinally, the processed data is formatted for database ingestion with appropriate metadata tags and quality indicators. This standardized approach ensures consistent data structure across expeditions, facilitating long-term monitoring and comparative analysis of marine ecosystems worldwide.\n\n\n\nExploratory Data Analysis\nAfter completion of the data processing pipeline, we employ a separate eda.qmd file to conduct standardized yet flexible exploratory analyses. This approach balances consistency across expeditions with the need for location-specific insights.\nOur EDA workflow includes several core analyses that we produce for every expedition:\nBiodiversity Assessment - Species accumulation curves to evaluate sampling adequacy - Diversity indices (Shannon, Simpson) compared across sites - Species rank-abundance distributions to characterize community structure\nStatistical Comparisons - Tests for significant differences in diversity and cover metrics - Analyses stratified by region, habitat type, and depth - Focus on key functional groups (hard corals, macroalgae, cyanobacteria)\nCommunity Composition - Multivariate analyses of benthic communities (NMDS, cluster analysis) - Visualization of functional group proportions using consistent color palettes - Identification of indicator species for different habitat types\nCoral Health - Bleaching prevalence by species and depth - Disease incidence and severity metrics\nSpatial Visualization - Interactive maps of survey sites with embedded metrics - Geographic pattern analysis",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#limitations",
    "href": "field-methods/benthic_lpi.html#limitations",
    "title": "Line Point Intercept",
    "section": "Limitations",
    "text": "Limitations\nWhile the Line Point Intercept (LPI) method offers efficiency and standardization for coral reef monitoring, several methodological trade-offs warrant consideration:\n\nPoint spacing\nSome research suggests 20cm intervals may be too close. Studies comparing intervals found 50cm spacing reduces survey time while still detecting major cover trends, though at the cost of potentially missing rarer species (Facon et al., 2016; Kuo et al., 2022). Statistical power analyses indicate substantially larger sample sizes than typically used are needed to detect even moderate changes in cover regardless of spacing (Leujak & Ormond, 2007).\n\n\nTransect length\nThe standard 50m transect represents a compromise between survey effort and habitat representation. Multiple shorter transects often provide better representation of reef spatial heterogeneity than fewer longer ones (Vallès et al., 2019). Linear methods inherently sample only a tiny proportion of reef area compared to quadrat approaches (0.33% vs 17.8% in some studies).\n\n\nAlternative methods\nFor bleaching assessment specifically, quadrat methods have demonstrated advantages in capturing colony-level impacts and detecting more species than point methods (Brown et al., 2015; Coral Reef Alliance, 2024). Belt transects provide a reasonable compromise for characterizing both coral cover and condition. We continue using LPI methods for consistency with historical data while acknowledging these limitations in our analyses and reports.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  },
  {
    "objectID": "field-methods/benthic_lpi.html#references",
    "href": "field-methods/benthic_lpi.html#references",
    "title": "Line Point Intercept",
    "section": "References",
    "text": "References\nCoral Reef Alliance. (2024). Coral Bleaching On-Site Monitoring Tools.\nDarling ES, Alvarez-Filip L, Oliver TA, McClanahan TR, Côté IM. (2012). Evaluating life-history strategies of reef corals from species traits. Ecology Letters, 15(12), 1378-1386.\nFacon, M., et al. (2016). A comparative study of the accuracy and effectiveness of Line and Point Intercept Transect methods for coral reef monitoring in the southwestern Indian Ocean islands. Ecological Indicators, 60, 1045-1055.\nKuo, C-Y., et al. (2022). Fine intervals are required when using point intercept transects to assess coral reef status. Frontiers in Marine Science, 9, 795512.\nLeujak, W., & Ormond, R.F.G. (2007). Comparative accuracy and efficiency of six coral community survey methods. Journal of Experimental Marine Biology and Ecology, 351(1-2), 168-187.\nVallès, H., et al. (2019). Switching between standard coral reef benthic monitoring protocols is complicated: proof of concept. PeerJ, 7, e8167.",
    "crumbs": [
      "Home",
      "Field Methods",
      "Underwater Visual Surveys (UVS)",
      "Benthos"
    ]
  }
]